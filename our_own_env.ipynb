{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import gym, ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents import ppo, ddpg\n",
    "from ray.tune import register_env\n",
    "\n",
    "from src.environments.create_env import create_env\n",
    "from src.environments.gym_power_voltage_env import GymPowerVoltageEnv\n",
    "from src.samplers.load_samplers import load_samplers\n",
    "\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'path_to_data':   'data/',\n",
    "          't0_hr': 6.,  # When the episode start (default value 6AM)\n",
    "          'dt_min': 30,  # Timestep size\n",
    "          'ev_dt_min': 60,  # Timestep size for EV arrivals\n",
    "          'ev_sampling_dt_min': 60,  # How EV sessions are sampled from the data\n",
    "          'apply_gaussian_noise': False,  # Make data noisy\n",
    "          'ev_utility_coef_mean': 1,  # Mean value of the utility coefficient for the EVs\n",
    "          'ev_utility_coef_scale': 0.13,  # STD of the utility coefficient for the EVs\n",
    "          'days_per_month_train': 20,  # Days per month for training\n",
    "          'ev_session_months_train': ['01', '02', '03', '04', '06', '07', '08', '09', '10', '11', ],\n",
    "          # Months to sample EV sessions for training\n",
    "          'grid_to_use': 'ieee16',  # What grid topology to use. Now supports only IEEE16.\n",
    "          'ev_session_months_test': ['05', '12'],  # Months to sample EV sessions for test\n",
    "          'n_ps_pvs': 4,  # Amount of solar panels that use PecanStreet data\n",
    "          'n_canopy_pvs': 0,  # Amount of solar panels that use canopy data\n",
    "          'canopy_pv_rated_power': 250,  # Rated power of these panels\n",
    "          'n_loads': 0,  # Amount of inflexible loads\n",
    "          'n_feeders': 1,  # Amount of feeders\n",
    "          'n_ev_chargers': 4,  # Amount of EV chargers\n",
    "\n",
    "          'ps_pvs_rated_power': 4,  # Rated power of these panels\n",
    "          'avg_evs_per_day': 3.5,  # Scaling of the EV arrival rate\n",
    "          'feeder_p_min': -5,  # Capacity of the feeders\n",
    "          'g': 4,  # Conductance of each line\n",
    "          'i_max': 25,  # Capacity of each line\n",
    "\n",
    "          'environment_type': 'gym',\n",
    "          }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(a):\n",
    "    # Preload samplers, it is necessary to avoid re-loading data each time env is created\n",
    "    (ps_samplers_dict, ps_metadata, canopy_sampler, canopy_metadata,\n",
    "     price_sampler, price_metadata, ev_sampler, elaadnl_metadata) = load_samplers(config)\n",
    "\n",
    "    return create_env(\n",
    "        config,\n",
    "        ps_samplers_dict,\n",
    "        ps_metadata,\n",
    "        canopy_sampler,\n",
    "        canopy_metadata,\n",
    "        price_sampler,\n",
    "        price_metadata,\n",
    "        ev_sampler,\n",
    "        elaadnl_metadata\n",
    "    )  # return an env instance\n",
    "\n",
    "\n",
    "# Read this on how to run our own environments\n",
    "# https://docs.ray.io/en/latest/rllib/rllib-env.html\n",
    "\n",
    "# ray.init()\n",
    "register_env(\"my_env\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(th.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 12:55:57,323\tINFO simple_q.py:161 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n",
      "2022-05-25 12:55:57,326\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(RolloutWorker pid=3164)\u001B[0m {'path_to_data': 'data/', 't0_hr': 6.0, 'dt_min': 30, 'ev_dt_min': 60, 'ev_sampling_dt_min': 60, 'apply_gaussian_noise': False, 'ev_utility_coef_mean': 1, 'ev_utility_coef_scale': 0.13, 'days_per_month_train': 20, 'ev_session_months_train': ['01', '02', '03', '04', '06', '07', '08', '09', '10', '11'], 'grid_to_use': 'ieee16', 'ev_session_months_test': ['05', '12'], 'n_ps_pvs': 4, 'n_canopy_pvs': 0, 'canopy_pv_rated_power': 250, 'n_loads': 0, 'n_feeders': 1, 'n_ev_chargers': 4, 'ps_pvs_rated_power': 4, 'avg_evs_per_day': 3.5, 'feeder_p_min': -5, 'g': 4, 'i_max': 25, 'environment_type': 'gym'}\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=3164)\u001B[0m loading pecanstreet\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=3164)\u001B[0m loading pvdata\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=3164)\u001B[0m loading elaadnl\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=3164)\u001B[0m loading newyork_price\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(RolloutWorker pid=3164)\u001B[0m 2022-05-25 12:58:22,959\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=3164)\u001B[0m 2022-05-25 12:58:22,959\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "C:\\Users\\Sjoerd Groot\\.conda\\envs\\RL-EVCP\\lib\\site-packages\\ray\\rllib\\agents\\ddpg\\ddpg_torch_model.py:110: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:178.)\n",
      "  torch.from_numpy(self.action_space.low).float()\n",
      "2022-05-25 12:58:25,850\tINFO trainable.py:152 -- Trainable.setup took 148.532 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-05-25 12:58:25,854\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'episode_reward_max': -7755.0,\n 'episode_reward_min': -8315.808311700821,\n 'episode_reward_mean': -8019.178993504855,\n 'episode_len_mean': 47.0,\n 'episode_media': {},\n 'episodes_this_iter': 31,\n 'policy_reward_min': {},\n 'policy_reward_max': {},\n 'policy_reward_mean': {},\n 'custom_metrics': {},\n 'hist_stats': {'episode_reward': [-8027.586515247822,\n   -8315.808311700821,\n   -8189.849503338337,\n   -8240.037979483604,\n   -8007.36781001091,\n   -8142.452227175236,\n   -8166.524214744568,\n   -8141.8783454597,\n   -8054.372584074736,\n   -7956.5983410179615,\n   -8257.948513627052,\n   -8184.76120531559,\n   -8210.522580891848,\n   -8251.272272616625,\n   -8127.221717238426,\n   -8222.58759751916,\n   -8066.885175853968,\n   -8105.836598575115,\n   -8123.126549571753,\n   -8042.244348675013,\n   -8146.273148953915,\n   -7817.523749470711,\n   -7755.0,\n   -7755.0,\n   -7755.0,\n   -7755.0,\n   -7755.0,\n   -7755.0,\n   -7755.0,\n   -7755.869508087635,\n   -7755.0],\n  'episode_lengths': [47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47]},\n 'sampler_perf': {'mean_raw_obs_processing_ms': 2.6723865188812113,\n  'mean_inference_ms': 1.3571339873454327,\n  'mean_action_processing_ms': 0.15592972172807654,\n  'mean_env_wait_ms': 1.435999072924366,\n  'mean_env_render_ms': 0.0},\n 'off_policy_estimator': {},\n 'num_healthy_workers': 1,\n 'timesteps_total': 1500,\n 'timesteps_this_iter': 256,\n 'agent_timesteps_total': 1500,\n 'timers': {'load_time_ms': 3.003,\n  'load_throughput': 85244.667,\n  'learn_time_ms': 30.998,\n  'learn_throughput': 8258.727,\n  'update_time_ms': 6.999},\n 'info': {'learner': {'default_policy': {'custom_metrics': {},\n    'learner_stats': {'actor_loss': 166.74693298339844,\n     'critic_loss': 14208.25863198285,\n     'mean_q': -163.65762329101562,\n     'max_q': -142.423828125,\n     'min_q': -176.19720458984375},\n    'model': {},\n    'td_error': array([-2.4289703e+00,  1.5927701e+02,  1.6431134e+02,  1.6296503e+02,\n            1.9326109e+02,  1.7135976e+02,  1.5755725e+02,  1.6511380e+02,\n            1.6348814e+02,  1.6364615e+02,  1.6330177e+02,  1.6818214e+02,\n            1.6346153e+02,  1.6368425e+02,  1.6523502e+02,  1.6296503e+02,\n            1.8246706e+02,  1.6326173e+02,  1.9326109e+02,  2.1600793e+02,\n            1.6352115e+02,  1.6376135e+02,  1.6329831e+02,  1.6250595e+02,\n            1.6301930e+02,  1.9140524e+02,  1.6398990e+02,  1.8100114e+02,\n            1.8099985e+02,  1.6279889e+02, -1.0941620e+00,  1.8311594e+02,\n            1.7474692e+02,  1.8627754e+02,  1.6394914e+02,  1.6345786e+02,\n            1.6948978e+02,  1.6409816e+02,  1.7188045e+02,  1.7200717e+02,\n            1.8330679e+02,  1.6404453e+02,  1.5739810e+02,  1.8251067e+02,\n            1.6124232e+02,  1.6353137e+02,  1.8789874e+02,  1.6377736e+02,\n            1.6341928e+02,  1.8743816e+02,  1.4683858e+02,  1.6382323e+02,\n            1.7600314e+02,  1.6246558e+02,  1.4243243e+02,  1.6287193e+02,\n            1.7408817e+02,  1.7414166e+02,  1.8515894e+02,  1.6609427e+02,\n            1.6506393e+02,  1.6328304e+02,  1.6347945e+02,  1.6879756e+02,\n            1.4637012e+02,  1.6347350e+02,  1.6370447e+02,  2.1067159e+02,\n            1.8246706e+02,  1.7703209e+02,  1.8485724e+02,  1.8578378e+02,\n            1.6282410e+02,  1.6293314e+02,  1.7291364e+02,  1.6643181e+02,\n            1.6348721e+02, -1.7280579e-01,  1.6203300e+02,  1.7559818e+02,\n            1.5801062e+02,  1.8762610e+02,  1.6346669e+02,  1.6361250e+02,\n            1.6402556e+02,  1.6343913e+02,  1.5158466e+02,  1.6343779e+02,\n            1.7248590e+02,  2.0711317e+02,  1.6118762e+02,  1.6328070e+02,\n            1.6820988e+02,  1.8513168e+02,  1.5651086e+02,  1.6318741e+02,\n            1.8388750e+02,  1.6312248e+02,  1.9134317e+02,  1.8766840e+02,\n            1.7433183e+02,  1.6621864e+02,  1.9378427e+02,  1.7048825e+02,\n            1.6451924e+02,  1.7861526e+02,  1.6398769e+02,  1.6899365e+02,\n            1.7560994e+02,  1.6275314e+02,  1.6899481e+02,  1.6707536e+02,\n            1.8515894e+02,  1.6379913e+02,  1.6574612e+02,  1.6363095e+02,\n            1.7419405e+02,  1.7119699e+02,  1.6201308e+02,  1.7484946e+02,\n            2.0015196e+02,  1.6259172e+02,  1.8436993e+02,  1.6533531e+02,\n            1.6151167e+02,  1.7071852e+02,  1.6343913e+02,  1.8518794e+02,\n            1.6314767e+02,  1.7194598e+02, -1.1120300e+00,  1.7134962e+02,\n            1.4279903e+02,  1.7858243e+02,  1.6340410e+02,  1.7396315e+02,\n            2.0794652e+02,  1.6751595e+02,  1.7620892e+02,  1.6353137e+02,\n            1.6428790e+02,  1.7125220e+02,  1.6358653e+02,  1.5319901e+02,\n            1.8261673e+02,  1.7682451e+02,  1.6410017e+02,  1.6342570e+02,\n            1.9961856e+02,  1.6879756e+02,  1.6421609e+02,  1.8865170e+02,\n            1.6831636e+02,  1.6438965e+02,  1.7621957e+02,  1.6367261e+02,\n            1.7344136e+02,  1.9252632e+02,  1.6372719e+02,  1.7414166e+02,\n            1.6853433e+02,  1.8230965e+02,  1.3849371e+02,  1.8333646e+02,\n            1.5751732e+02,  1.5943448e+02,  1.6326581e+02,  1.6347577e+02,\n            1.6937474e+02,  1.6818102e+02,  1.8067393e+02,  1.5647560e+02,\n            1.6215012e+02,  1.8513168e+02,  1.9130730e+02,  1.8609108e+02,\n            1.6438965e+02,  1.8891624e+02,  1.6321104e+02,  1.5564520e+02,\n            1.6682777e+02,  1.6402556e+02,  1.8648032e+02,  1.8833011e+02,\n            1.1099899e+01,  1.8259082e+02,  1.8677122e+02,  1.6180489e+02,\n            1.8298151e+02,  1.6281105e+02,  1.7184047e+02,  1.7394594e+02,\n            1.7186989e+02,  1.6340877e+02,  1.6275314e+02,  1.8248308e+02,\n            2.0641684e+02,  1.7205774e+02,  1.6171153e+02,  1.6370956e+02,\n            1.6595348e+02,  1.4928299e+02,  1.7584984e+02,  2.9875488e+00,\n            1.6469786e+02,  1.7672812e+02,  1.6391188e+02,  1.6470227e+02,\n           -1.2166443e+00,  1.7071175e+02,  1.9249263e+02,  1.7560994e+02,\n            1.6318494e+02,  1.6427676e+02,  1.7417130e+02,  1.7304466e+02,\n            1.6915736e+02,  1.3802199e+02,  1.8751768e+02,  1.6438965e+02,\n            1.6368356e+02,  1.8012910e+02,  1.7519809e+02,  1.6327823e+02,\n            1.2854343e+02,  1.8015958e+02,  1.7655527e+02,  1.7882063e+02,\n            1.6396980e+02,  1.7364967e+02,  1.6328070e+02,  1.7681239e+02,\n            1.6483168e+02,  1.9140524e+02,  1.6346153e+02,  1.7879909e+02,\n            1.5887242e+02,  1.9584003e+02, -1.7280579e-01,  1.7408817e+02,\n            1.6323761e+02,  1.6419228e+02,  2.0015196e+02,  1.7296942e+02,\n            1.6396242e+02,  1.6483427e+02,  1.5414490e+02,  1.7887903e+02,\n            1.4712817e+02,  1.6427676e+02,  1.8432465e+02,  1.8177232e+02,\n            1.6459355e+02,  1.9735497e+02,  1.6374367e+02,  1.6326535e+02],\n          dtype=float32),\n    'mean_td_error': 165.48971557617188}},\n  'num_steps_sampled': 1500,\n  'num_agent_steps_sampled': 1500,\n  'num_steps_trained': 256,\n  'num_steps_trained_this_iter': 256,\n  'num_agent_steps_trained': 256,\n  'last_target_update_ts': 1500,\n  'num_target_updates': 1},\n 'done': False,\n 'episodes_total': 31,\n 'training_iteration': 1,\n 'trial_id': 'default',\n 'experiment_id': '40b4f75474f74cceb3b69a0d9d0c8c3d',\n 'date': '2022-05-25_12-58-38',\n 'timestamp': 1653476318,\n 'time_this_iter_s': 12.585428237915039,\n 'time_total_s': 12.585428237915039,\n 'pid': 4828,\n 'hostname': 'DESKTOP-M33545T',\n 'node_ip': '127.0.0.1',\n 'config': {'num_workers': 1,\n  'num_envs_per_worker': 1,\n  'create_env_on_driver': False,\n  'rollout_fragment_length': 1,\n  'batch_mode': 'truncate_episodes',\n  'gamma': 0.99,\n  'lr': 0.0001,\n  'train_batch_size': 256,\n  'model': {'_use_default_native_models': False,\n   '_disable_preprocessor_api': False,\n   '_disable_action_flattening': False,\n   'fcnet_hiddens': [256, 256],\n   'fcnet_activation': 'tanh',\n   'conv_filters': None,\n   'conv_activation': 'relu',\n   'post_fcnet_hiddens': [],\n   'post_fcnet_activation': 'relu',\n   'free_log_std': False,\n   'no_final_linear': False,\n   'vf_share_layers': True,\n   'use_lstm': False,\n   'max_seq_len': 20,\n   'lstm_cell_size': 256,\n   'lstm_use_prev_action': False,\n   'lstm_use_prev_reward': False,\n   '_time_major': False,\n   'use_attention': False,\n   'attention_num_transformer_units': 1,\n   'attention_dim': 64,\n   'attention_num_heads': 1,\n   'attention_head_dim': 32,\n   'attention_memory_inference': 50,\n   'attention_memory_training': 50,\n   'attention_position_wise_mlp_dim': 32,\n   'attention_init_gru_gate_bias': 2.0,\n   'attention_use_n_prev_actions': 0,\n   'attention_use_n_prev_rewards': 0,\n   'framestack': True,\n   'dim': 84,\n   'grayscale': False,\n   'zero_mean': True,\n   'custom_model': None,\n   'custom_model_config': {},\n   'custom_action_dist': None,\n   'custom_preprocessor': None,\n   'lstm_use_prev_action_reward': -1},\n  'optimizer': {},\n  'horizon': None,\n  'soft_horizon': False,\n  'no_done_at_end': False,\n  'env': 'my_env',\n  'observation_space': None,\n  'action_space': None,\n  'env_config': {},\n  'remote_worker_envs': False,\n  'remote_env_batch_wait_ms': 0,\n  'env_task_fn': None,\n  'render_env': False,\n  'record_env': False,\n  'clip_rewards': None,\n  'normalize_actions': True,\n  'clip_actions': False,\n  'preprocessor_pref': 'deepmind',\n  'log_level': 'WARN',\n  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n  'ignore_worker_failures': False,\n  'log_sys_usage': True,\n  'fake_sampler': False,\n  'framework': 'torch',\n  'eager_tracing': False,\n  'eager_max_retraces': 20,\n  'explore': True,\n  'exploration_config': {'type': 'OrnsteinUhlenbeckNoise',\n   'random_timesteps': 1000,\n   'ou_base_scale': 0.1,\n   'ou_theta': 0.15,\n   'ou_sigma': 0.2,\n   'initial_scale': 1.0,\n   'final_scale': 0.02,\n   'scale_timesteps': 10000},\n  'evaluation_interval': None,\n  'evaluation_duration': 10,\n  'evaluation_duration_unit': 'episodes',\n  'evaluation_parallel_to_training': False,\n  'in_evaluation': False,\n  'evaluation_config': {'num_workers': 1,\n   'num_envs_per_worker': 1,\n   'create_env_on_driver': False,\n   'rollout_fragment_length': 1,\n   'batch_mode': 'truncate_episodes',\n   'gamma': 0.99,\n   'lr': 0.0001,\n   'train_batch_size': 256,\n   'model': {'_use_default_native_models': False,\n    '_disable_preprocessor_api': False,\n    '_disable_action_flattening': False,\n    'fcnet_hiddens': [256, 256],\n    'fcnet_activation': 'tanh',\n    'conv_filters': None,\n    'conv_activation': 'relu',\n    'post_fcnet_hiddens': [],\n    'post_fcnet_activation': 'relu',\n    'free_log_std': False,\n    'no_final_linear': False,\n    'vf_share_layers': True,\n    'use_lstm': False,\n    'max_seq_len': 20,\n    'lstm_cell_size': 256,\n    'lstm_use_prev_action': False,\n    'lstm_use_prev_reward': False,\n    '_time_major': False,\n    'use_attention': False,\n    'attention_num_transformer_units': 1,\n    'attention_dim': 64,\n    'attention_num_heads': 1,\n    'attention_head_dim': 32,\n    'attention_memory_inference': 50,\n    'attention_memory_training': 50,\n    'attention_position_wise_mlp_dim': 32,\n    'attention_init_gru_gate_bias': 2.0,\n    'attention_use_n_prev_actions': 0,\n    'attention_use_n_prev_rewards': 0,\n    'framestack': True,\n    'dim': 84,\n    'grayscale': False,\n    'zero_mean': True,\n    'custom_model': None,\n    'custom_model_config': {},\n    'custom_action_dist': None,\n    'custom_preprocessor': None,\n    'lstm_use_prev_action_reward': -1},\n   'optimizer': {},\n   'horizon': None,\n   'soft_horizon': False,\n   'no_done_at_end': False,\n   'env': 'my_env',\n   'observation_space': None,\n   'action_space': None,\n   'env_config': {},\n   'remote_worker_envs': False,\n   'remote_env_batch_wait_ms': 0,\n   'env_task_fn': None,\n   'render_env': False,\n   'record_env': False,\n   'clip_rewards': None,\n   'normalize_actions': True,\n   'clip_actions': False,\n   'preprocessor_pref': 'deepmind',\n   'log_level': 'WARN',\n   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n   'ignore_worker_failures': False,\n   'log_sys_usage': True,\n   'fake_sampler': False,\n   'framework': 'torch',\n   'eager_tracing': False,\n   'eager_max_retraces': 20,\n   'explore': False,\n   'exploration_config': {'type': 'OrnsteinUhlenbeckNoise',\n    'random_timesteps': 1000,\n    'ou_base_scale': 0.1,\n    'ou_theta': 0.15,\n    'ou_sigma': 0.2,\n    'initial_scale': 1.0,\n    'final_scale': 0.02,\n    'scale_timesteps': 10000},\n   'evaluation_interval': None,\n   'evaluation_duration': 10,\n   'evaluation_duration_unit': 'episodes',\n   'evaluation_parallel_to_training': False,\n   'in_evaluation': False,\n   'evaluation_config': {'explore': False},\n   'evaluation_num_workers': 0,\n   'custom_eval_function': None,\n   'always_attach_evaluation_results': False,\n   'keep_per_episode_custom_metrics': False,\n   'sample_async': False,\n   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n   'observation_filter': 'NoFilter',\n   'synchronize_filters': True,\n   'tf_session_args': {'intra_op_parallelism_threads': 2,\n    'inter_op_parallelism_threads': 2,\n    'gpu_options': {'allow_growth': True},\n    'log_device_placement': False,\n    'device_count': {'CPU': 1},\n    'allow_soft_placement': True},\n   'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n    'inter_op_parallelism_threads': 8},\n   'compress_observations': False,\n   'metrics_episode_collection_timeout_s': 180,\n   'metrics_num_episodes_for_smoothing': 100,\n   'min_time_s_per_reporting': 1,\n   'min_train_timesteps_per_reporting': None,\n   'min_sample_timesteps_per_reporting': 1000,\n   'seed': None,\n   'extra_python_environs_for_driver': {},\n   'extra_python_environs_for_worker': {},\n   'num_gpus': 1,\n   '_fake_gpus': False,\n   'num_cpus_per_worker': 1,\n   'num_gpus_per_worker': 0,\n   'custom_resources_per_worker': {},\n   'num_cpus_for_driver': 1,\n   'placement_strategy': 'PACK',\n   'input': 'sampler',\n   'input_config': {},\n   'actions_in_input_normalized': False,\n   'input_evaluation': ['is', 'wis'],\n   'postprocess_inputs': False,\n   'shuffle_buffer_size': 0,\n   'output': None,\n   'output_config': {},\n   'output_compress_columns': ['obs', 'new_obs'],\n   'output_max_file_size': 67108864,\n   'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.policy_template.DDPGTorchPolicy'>, observation_space=Box([ -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.\n       -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.   0.   0.   0.   0.   0.   0.\n        0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n        0.   0. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300.\n      300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 400. 400. 400. 400.\n      400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400.\n      400. 400. 400. 400.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.\n       -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.], [  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n        0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   10.   10.\n       10.   10.   10.   10.   10.   10.   10.   10.   10.   10.   10.   10.\n       10.   10.   10.   10.   10.   10.   10.   10.  300.  300.  300.  300.\n      300.  300.  300.  300.  300.  300.  300.  300.  300.  300.  300.  300.\n      300.  300.  300.  300.  300.  300.  400.  400.  400.  400.  400.  400.\n      400.  400.  400.  400.  400.  400.  400.  400.  400.  400.  400.  400.\n      400.  400.  400.  400.    1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5\n        1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5\n        1.5   1.5], (110,), float64), action_space=Box([ -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.\n       -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5. 300. 300. 300. 300. 300. 300.\n      300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300.\n      300. 300.], [ 10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.\n       10.  10.  10.  10.  10.  10.  10.  10. 400. 400. 400. 400. 400. 400.\n      400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400.\n      400. 400.], (44,), float64), config={})},\n    'policy_map_capacity': 100,\n    'policy_map_cache': None,\n    'policy_mapping_fn': None,\n    'policies_to_train': None,\n    'observation_fn': None,\n    'replay_mode': 'independent',\n    'count_steps_by': 'env_steps'},\n   'logger_config': None,\n   '_tf_policy_handles_more_than_one_loss': False,\n   '_disable_preprocessor_api': False,\n   '_disable_action_flattening': False,\n   '_disable_execution_plan_api': False,\n   'disable_env_checking': False,\n   'simple_optimizer': False,\n   'monitor': -1,\n   'evaluation_num_episodes': -1,\n   'metrics_smoothing_episodes': -1,\n   'timesteps_per_iteration': 1000,\n   'min_iter_time_s': -1,\n   'collect_metrics_timeout': -1,\n   'twin_q': False,\n   'policy_delay': 1,\n   'smooth_target_policy': False,\n   'target_noise': 0.2,\n   'target_noise_clip': 0.5,\n   'use_state_preprocessor': False,\n   'actor_hiddens': [400, 300],\n   'actor_hidden_activation': 'relu',\n   'critic_hiddens': [400, 300],\n   'critic_hidden_activation': 'relu',\n   'n_step': 1,\n   'buffer_size': -1,\n   'replay_buffer_config': {'type': 'MultiAgentReplayBuffer',\n    'capacity': 50000},\n   'store_buffer_in_checkpoints': False,\n   'prioritized_replay': True,\n   'prioritized_replay_alpha': 0.6,\n   'prioritized_replay_beta': 0.4,\n   'prioritized_replay_beta_annealing_timesteps': 20000,\n   'final_prioritized_replay_beta': 0.4,\n   'prioritized_replay_eps': 1e-06,\n   'training_intensity': None,\n   'critic_lr': 0.001,\n   'actor_lr': 0.001,\n   'target_network_update_freq': 0,\n   'tau': 0.002,\n   'use_huber': False,\n   'huber_threshold': 1.0,\n   'l2_reg': 1e-06,\n   'grad_clip': None,\n   'learning_starts': 1500,\n   'worker_side_prioritization': False},\n  'evaluation_num_workers': 0,\n  'custom_eval_function': None,\n  'always_attach_evaluation_results': False,\n  'keep_per_episode_custom_metrics': False,\n  'sample_async': False,\n  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n  'observation_filter': 'NoFilter',\n  'synchronize_filters': True,\n  'tf_session_args': {'intra_op_parallelism_threads': 2,\n   'inter_op_parallelism_threads': 2,\n   'gpu_options': {'allow_growth': True},\n   'log_device_placement': False,\n   'device_count': {'CPU': 1},\n   'allow_soft_placement': True},\n  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n   'inter_op_parallelism_threads': 8},\n  'compress_observations': False,\n  'metrics_episode_collection_timeout_s': 180,\n  'metrics_num_episodes_for_smoothing': 100,\n  'min_time_s_per_reporting': 1,\n  'min_train_timesteps_per_reporting': None,\n  'min_sample_timesteps_per_reporting': 1000,\n  'seed': None,\n  'extra_python_environs_for_driver': {},\n  'extra_python_environs_for_worker': {},\n  'num_gpus': 1,\n  '_fake_gpus': False,\n  'num_cpus_per_worker': 1,\n  'num_gpus_per_worker': 0,\n  'custom_resources_per_worker': {},\n  'num_cpus_for_driver': 1,\n  'placement_strategy': 'PACK',\n  'input': 'sampler',\n  'input_config': {},\n  'actions_in_input_normalized': False,\n  'input_evaluation': ['is', 'wis'],\n  'postprocess_inputs': False,\n  'shuffle_buffer_size': 0,\n  'output': None,\n  'output_config': {},\n  'output_compress_columns': ['obs', 'new_obs'],\n  'output_max_file_size': 67108864,\n  'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.policy_template.DDPGTorchPolicy'>, observation_space=Box([ -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.\n      -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.   0.   0.   0.   0.   0.   0.\n       0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n       0.   0. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300.\n     300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 400. 400. 400. 400.\n     400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400.\n     400. 400. 400. 400.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.\n      -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.], [  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n       0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   10.   10.\n      10.   10.   10.   10.   10.   10.   10.   10.   10.   10.   10.   10.\n      10.   10.   10.   10.   10.   10.   10.   10.  300.  300.  300.  300.\n     300.  300.  300.  300.  300.  300.  300.  300.  300.  300.  300.  300.\n     300.  300.  300.  300.  300.  300.  400.  400.  400.  400.  400.  400.\n     400.  400.  400.  400.  400.  400.  400.  400.  400.  400.  400.  400.\n     400.  400.  400.  400.    1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5\n       1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5\n       1.5   1.5], (110,), float64), action_space=Box([ -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.\n      -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5. 300. 300. 300. 300. 300. 300.\n     300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300.\n     300. 300.], [ 10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.\n      10.  10.  10.  10.  10.  10.  10.  10. 400. 400. 400. 400. 400. 400.\n     400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400.\n     400. 400.], (44,), float64), config={})},\n   'policy_map_capacity': 100,\n   'policy_map_cache': None,\n   'policy_mapping_fn': None,\n   'policies_to_train': None,\n   'observation_fn': None,\n   'replay_mode': 'independent',\n   'count_steps_by': 'env_steps'},\n  'logger_config': None,\n  '_tf_policy_handles_more_than_one_loss': False,\n  '_disable_preprocessor_api': False,\n  '_disable_action_flattening': False,\n  '_disable_execution_plan_api': False,\n  'disable_env_checking': False,\n  'simple_optimizer': False,\n  'monitor': -1,\n  'evaluation_num_episodes': -1,\n  'metrics_smoothing_episodes': -1,\n  'timesteps_per_iteration': 1000,\n  'min_iter_time_s': -1,\n  'collect_metrics_timeout': -1,\n  'twin_q': False,\n  'policy_delay': 1,\n  'smooth_target_policy': False,\n  'target_noise': 0.2,\n  'target_noise_clip': 0.5,\n  'use_state_preprocessor': False,\n  'actor_hiddens': [400, 300],\n  'actor_hidden_activation': 'relu',\n  'critic_hiddens': [400, 300],\n  'critic_hidden_activation': 'relu',\n  'n_step': 1,\n  'buffer_size': -1,\n  'replay_buffer_config': {'type': 'MultiAgentReplayBuffer',\n   'capacity': 50000},\n  'store_buffer_in_checkpoints': False,\n  'prioritized_replay': True,\n  'prioritized_replay_alpha': 0.6,\n  'prioritized_replay_beta': 0.4,\n  'prioritized_replay_beta_annealing_timesteps': 20000,\n  'final_prioritized_replay_beta': 0.4,\n  'prioritized_replay_eps': 1e-06,\n  'training_intensity': None,\n  'critic_lr': 0.001,\n  'actor_lr': 0.001,\n  'target_network_update_freq': 0,\n  'tau': 0.002,\n  'use_huber': False,\n  'huber_threshold': 1.0,\n  'l2_reg': 1e-06,\n  'grad_clip': None,\n  'learning_starts': 1500,\n  'worker_side_prioritization': False},\n 'time_since_restore': 12.585428237915039,\n 'timesteps_since_restore': 256,\n 'iterations_since_restore': 1,\n 'warmup_time': 148.53814721107483,\n 'perf': {'cpu_util_percent': 19.344444444444445,\n  'ram_util_percent': 71.89444444444446}}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = ddpg.DDPGTrainer(\n",
    "    env=\"my_env\",\n",
    "    # Stopping condition\n",
    "    # stop={\"episode_reward_mean\":200},\n",
    "\n",
    "    # Config\n",
    "    # The default DDPG specific config is used with required \n",
    "    # Options for the config are in the default DDPG config: \n",
    "    # https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#ddpg\n",
    "    config={\n",
    "        \"env\": \"my_env\",\n",
    "        \"framework\": \"torch\",\n",
    "        \"num_gpus\":1,\n",
    "        \"num_workers\":1,\n",
    "    },\n",
    "    # checkpoint_freq=1\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# trainer = ppo.PPOTrainer(env=\"my_env\", config={\n",
    "#     \"env_config\": config,  # config to pass to env class\n",
    "#     \"framework\": \"torch\",\n",
    "# })\n",
    "\n",
    "# while True:\n",
    "#     print(trainer.train())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'episode_reward_max': -7050.0,\n 'episode_reward_min': -8315.808311700821,\n 'episode_reward_mean': -7633.387713182085,\n 'episode_len_mean': 47.0,\n 'episode_media': {},\n 'episodes_this_iter': 22,\n 'policy_reward_min': {},\n 'policy_reward_max': {},\n 'policy_reward_mean': {},\n 'custom_metrics': {},\n 'hist_stats': {'episode_reward': [-8027.586515247822,\n   -8315.808311700821,\n   -8189.849503338337,\n   -8240.037979483604,\n   -8007.36781001091,\n   -8142.452227175236,\n   -8166.524214744568,\n   -8141.8783454597,\n   -8054.372584074736,\n   -7956.5983410179615,\n   -8257.948513627052,\n   -8184.76120531559,\n   -8210.522580891848,\n   -8251.272272616625,\n   -8127.221717238426,\n   -8222.58759751916,\n   -8066.885175853968,\n   -8105.836598575115,\n   -8123.126549571753,\n   -8042.244348675013,\n   -8146.273148953915,\n   -7817.523749470711,\n   -7755.0,\n   -7755.0,\n   -7755.0,\n   -7755.0,\n   -7755.0,\n   -7755.0,\n   -7755.0,\n   -7755.869508087635,\n   -7755.0,\n   -7725.0,\n   -7250.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0],\n  'episode_lengths': [47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47]},\n 'sampler_perf': {'mean_raw_obs_processing_ms': 2.6864755522407293,\n  'mean_inference_ms': 1.3699575098388033,\n  'mean_action_processing_ms': 0.15624388677023243,\n  'mean_env_wait_ms': 1.4205040103316158,\n  'mean_env_render_ms': 0.0},\n 'off_policy_estimator': {},\n 'num_healthy_workers': 1,\n 'timesteps_total': 2500,\n 'timesteps_this_iter': 256,\n 'agent_timesteps_total': 2500,\n 'timers': {'load_time_ms': 2.595,\n  'load_throughput': 98657.769,\n  'learn_time_ms': 28.2,\n  'learn_throughput': 9077.974,\n  'update_time_ms': 5.001},\n 'info': {'learner': {'default_policy': {'custom_metrics': {},\n    'learner_stats': {'actor_loss': 608.1211547851562,\n     'critic_loss': 428.1100435899797,\n     'mean_q': -624.4853515625,\n     'max_q': -573.0040283203125,\n     'min_q': -684.0628662109375},\n    'model': {},\n    'td_error': array([-6.8217773e+00,  3.5521851e+00,  1.6059326e+01, -8.7738647e+00,\n           -1.0610779e+01, -2.0521240e+00, -6.4815063e+00, -6.4698486e+00,\n           -7.6166382e+00,  8.5255127e+00,  6.4744263e+00,  3.6510010e+00,\n            6.7993164e-01, -3.3263062e+01, -1.2608643e+01, -1.7597656e+00,\n           -1.3196350e+01,  1.8952026e+00, -4.0567627e+00,  4.8092651e+00,\n           -8.6517944e+00,  1.3322510e+01,  1.6908630e+01,  2.3515625e+01,\n            1.9987793e+01,  2.1138916e+00, -1.1639404e+00, -1.4693359e+01,\n            1.0022766e+01, -8.5292358e+00, -3.2040161e+01,  5.9582520e+00,\n            9.9545898e+00,  1.6918213e+01, -1.6332092e+01, -1.9236450e+00,\n           -1.8403198e+01, -3.4053223e+01,  7.9419556e+00,  2.3621826e+00,\n            8.0126343e+00, -5.4652710e+00,  4.5128296e+01,  4.8402710e+00,\n           -6.5234375e-01,  4.2088013e+00,  1.3056091e+01,  2.5288696e+00,\n            9.3222656e+00, -4.2681885e-01,  1.5328369e+00, -1.0567444e+01,\n           -6.5234375e-01, -1.1730835e+01, -1.4406799e+01,  1.4794739e+01,\n           -2.7824707e+00, -1.2261353e+00,  1.0759155e+01,  2.1508057e+01,\n            1.0696960e+01,  3.6939575e+01,  2.2552795e+01,  3.0051758e+01,\n            1.0302307e+01,  2.2547974e+01,  3.1204224e+00, -1.6958862e+01,\n            4.3469238e-01, -9.0968628e+00,  1.0103455e+01, -4.0046387e+00,\n           -2.5092773e+00, -1.6951904e+00,  1.6758728e+01, -1.8321472e+01,\n            1.0362366e+01,  1.6263000e+01,  2.2804077e+01, -1.1700256e+01,\n           -2.1418152e+01,  2.1076660e+00, -1.8890625e+01, -8.6948242e+00,\n           -1.5007935e+01,  3.7836060e+01,  1.1673889e+01,  6.3560791e+00,\n            2.3086182e+01,  1.1060425e+01,  5.9454346e-01,  2.3118530e+01,\n            1.9785400e+01, -1.0947510e+01, -7.7172852e-01,  3.7922729e+01,\n           -5.4652710e+00, -1.0062866e+00, -1.8032349e+01, -1.1891479e+00,\n            4.3025879e+01, -1.1886841e+01,  1.4595032e+01, -1.0305176e+00,\n           -3.6553955e+00,  1.9186829e+01, -8.4501953e+00,  1.6941284e+01,\n           -1.8495056e+01,  6.6026611e+00, -5.5312500e+00,  2.6055969e+01,\n            1.4824707e+01, -5.3853760e+00,  1.6531372e+01,  1.7437988e+01,\n           -1.5462341e+01,  5.0852051e+00, -1.8286316e+01,  3.7167969e+00,\n            1.0026794e+01, -9.7987061e+00, -7.9266357e-01,  1.3423828e+01,\n            2.9768677e+00, -1.4271423e+01,  6.9386597e+00,  5.3814697e-01,\n            1.7008911e+01, -6.9317017e+00,  6.4154663e+00,  1.2110718e+01,\n           -9.4066162e+00,  3.1361084e+00,  2.2600403e+01,  9.8602295e-01,\n           -5.7589722e+00,  3.2098145e+01,  7.4107666e+00, -2.8705444e+00,\n           -8.8336182e-01, -4.5344849e+00,  6.0241699e-01,  1.6474365e+01,\n            7.2601929e+00,  3.0548706e+01, -1.1717529e+00, -4.9194336e+00,\n            1.5224121e+01,  1.4684326e+01, -3.4835815e+00,  9.6596069e+00,\n            1.6952942e+01, -8.4171753e+00,  5.2324829e+00,  1.1083984e+00,\n           -1.6627930e+01, -1.4382568e+01,  5.4547119e+00, -1.9972290e+01,\n            2.3117676e+00, -7.5878906e+00,  1.4595032e+01,  1.6533081e+01,\n            9.8207397e+00, -7.6578369e+00, -2.1700439e+00, -4.1345892e+02,\n           -1.8286316e+01, -2.6986694e+00, -1.5270996e+00, -1.3386108e+01,\n           -1.5621948e+00, -5.5726929e+00,  5.0852051e+00,  9.4174194e+00,\n            1.0155334e+01, -4.9901123e+00,  9.3546143e+00, -5.6377563e+00,\n            2.1025513e+01, -6.0598145e+00, -7.9517822e+00, -2.7284912e+01,\n            1.6059326e+01,  9.7319336e+00,  1.3148193e+00, -1.6237610e+01,\n            8.9212036e+00, -6.7670288e+00, -5.2185669e+00, -6.9527588e+00,\n           -1.7772583e+01, -5.4652710e+00,  7.0816650e+00, -1.1717529e+00,\n           -5.9472656e-01,  8.0126343e+00, -2.0825256e+01,  1.7606812e+01,\n           -1.0604187e+01,  4.0787964e+00,  6.0704346e+00, -9.8712158e+00,\n            3.5727295e+01, -2.1060852e+01, -4.8525391e+00,  1.8258362e+01,\n            3.0554199e-01, -1.6332092e+01, -9.0968628e+00, -1.0645325e+01,\n           -1.0846497e+01, -3.6614990e-01,  1.7851440e+01,  3.2964478e+00,\n            7.3610840e+00,  1.1353516e+01, -1.1534607e+01, -1.0639160e+01,\n            1.8527893e+01,  2.5229126e+01,  1.3367737e+01,  1.2295105e+01,\n           -1.2917786e+01,  8.3709717e+00, -9.4497681e+00, -7.9517822e+00,\n            1.0191528e+01, -7.0610352e+00,  8.1176758e+00, -7.9376221e-01,\n            1.6758728e+01,  1.1948547e+01,  1.1083984e+00, -1.9067078e+01,\n            1.1584473e+01,  1.6623169e+01,  1.2221069e+00,  1.3399658e+00,\n            4.5930176e+00, -7.3034058e+00, -4.9834595e+00,  2.5316833e+01,\n           -1.7297363e-01,  9.0711670e+00,  4.0130615e+00, -1.0464172e+01,\n            1.2735596e+01,  2.2547974e+01,  6.5772095e+00,  1.8878784e+00,\n           -1.4406799e+01,  4.5501099e+00,  6.3398438e+00, -2.7941895e+00],\n          dtype=float32),\n    'mean_td_error': 0.6631536483764648}},\n  'num_steps_sampled': 2500,\n  'num_agent_steps_sampled': 2500,\n  'num_steps_trained': 256256,\n  'num_steps_trained_this_iter': 256,\n  'num_agent_steps_trained': 256256,\n  'last_target_update_ts': 2500,\n  'num_target_updates': 1001},\n 'done': False,\n 'episodes_total': 53,\n 'training_iteration': 2,\n 'trial_id': 'default',\n 'experiment_id': '40b4f75474f74cceb3b69a0d9d0c8c3d',\n 'date': '2022-05-25_13-01-24',\n 'timestamp': 1653476484,\n 'time_this_iter_s': 67.51772212982178,\n 'time_total_s': 80.10315036773682,\n 'pid': 4828,\n 'hostname': 'DESKTOP-M33545T',\n 'node_ip': '127.0.0.1',\n 'config': {'num_workers': 1,\n  'num_envs_per_worker': 1,\n  'create_env_on_driver': False,\n  'rollout_fragment_length': 1,\n  'batch_mode': 'truncate_episodes',\n  'gamma': 0.99,\n  'lr': 0.0001,\n  'train_batch_size': 256,\n  'model': {'_use_default_native_models': False,\n   '_disable_preprocessor_api': False,\n   '_disable_action_flattening': False,\n   'fcnet_hiddens': [256, 256],\n   'fcnet_activation': 'tanh',\n   'conv_filters': None,\n   'conv_activation': 'relu',\n   'post_fcnet_hiddens': [],\n   'post_fcnet_activation': 'relu',\n   'free_log_std': False,\n   'no_final_linear': False,\n   'vf_share_layers': True,\n   'use_lstm': False,\n   'max_seq_len': 20,\n   'lstm_cell_size': 256,\n   'lstm_use_prev_action': False,\n   'lstm_use_prev_reward': False,\n   '_time_major': False,\n   'use_attention': False,\n   'attention_num_transformer_units': 1,\n   'attention_dim': 64,\n   'attention_num_heads': 1,\n   'attention_head_dim': 32,\n   'attention_memory_inference': 50,\n   'attention_memory_training': 50,\n   'attention_position_wise_mlp_dim': 32,\n   'attention_init_gru_gate_bias': 2.0,\n   'attention_use_n_prev_actions': 0,\n   'attention_use_n_prev_rewards': 0,\n   'framestack': True,\n   'dim': 84,\n   'grayscale': False,\n   'zero_mean': True,\n   'custom_model': None,\n   'custom_model_config': {},\n   'custom_action_dist': None,\n   'custom_preprocessor': None,\n   'lstm_use_prev_action_reward': -1},\n  'optimizer': {},\n  'horizon': None,\n  'soft_horizon': False,\n  'no_done_at_end': False,\n  'env': 'my_env',\n  'observation_space': None,\n  'action_space': None,\n  'env_config': {},\n  'remote_worker_envs': False,\n  'remote_env_batch_wait_ms': 0,\n  'env_task_fn': None,\n  'render_env': False,\n  'record_env': False,\n  'clip_rewards': None,\n  'normalize_actions': True,\n  'clip_actions': False,\n  'preprocessor_pref': 'deepmind',\n  'log_level': 'WARN',\n  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n  'ignore_worker_failures': False,\n  'log_sys_usage': True,\n  'fake_sampler': False,\n  'framework': 'torch',\n  'eager_tracing': False,\n  'eager_max_retraces': 20,\n  'explore': True,\n  'exploration_config': {'type': 'OrnsteinUhlenbeckNoise',\n   'random_timesteps': 1000,\n   'ou_base_scale': 0.1,\n   'ou_theta': 0.15,\n   'ou_sigma': 0.2,\n   'initial_scale': 1.0,\n   'final_scale': 0.02,\n   'scale_timesteps': 10000},\n  'evaluation_interval': None,\n  'evaluation_duration': 10,\n  'evaluation_duration_unit': 'episodes',\n  'evaluation_parallel_to_training': False,\n  'in_evaluation': False,\n  'evaluation_config': {'num_workers': 1,\n   'num_envs_per_worker': 1,\n   'create_env_on_driver': False,\n   'rollout_fragment_length': 1,\n   'batch_mode': 'truncate_episodes',\n   'gamma': 0.99,\n   'lr': 0.0001,\n   'train_batch_size': 256,\n   'model': {'_use_default_native_models': False,\n    '_disable_preprocessor_api': False,\n    '_disable_action_flattening': False,\n    'fcnet_hiddens': [256, 256],\n    'fcnet_activation': 'tanh',\n    'conv_filters': None,\n    'conv_activation': 'relu',\n    'post_fcnet_hiddens': [],\n    'post_fcnet_activation': 'relu',\n    'free_log_std': False,\n    'no_final_linear': False,\n    'vf_share_layers': True,\n    'use_lstm': False,\n    'max_seq_len': 20,\n    'lstm_cell_size': 256,\n    'lstm_use_prev_action': False,\n    'lstm_use_prev_reward': False,\n    '_time_major': False,\n    'use_attention': False,\n    'attention_num_transformer_units': 1,\n    'attention_dim': 64,\n    'attention_num_heads': 1,\n    'attention_head_dim': 32,\n    'attention_memory_inference': 50,\n    'attention_memory_training': 50,\n    'attention_position_wise_mlp_dim': 32,\n    'attention_init_gru_gate_bias': 2.0,\n    'attention_use_n_prev_actions': 0,\n    'attention_use_n_prev_rewards': 0,\n    'framestack': True,\n    'dim': 84,\n    'grayscale': False,\n    'zero_mean': True,\n    'custom_model': None,\n    'custom_model_config': {},\n    'custom_action_dist': None,\n    'custom_preprocessor': None,\n    'lstm_use_prev_action_reward': -1},\n   'optimizer': {},\n   'horizon': None,\n   'soft_horizon': False,\n   'no_done_at_end': False,\n   'env': 'my_env',\n   'observation_space': None,\n   'action_space': None,\n   'env_config': {},\n   'remote_worker_envs': False,\n   'remote_env_batch_wait_ms': 0,\n   'env_task_fn': None,\n   'render_env': False,\n   'record_env': False,\n   'clip_rewards': None,\n   'normalize_actions': True,\n   'clip_actions': False,\n   'preprocessor_pref': 'deepmind',\n   'log_level': 'WARN',\n   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n   'ignore_worker_failures': False,\n   'log_sys_usage': True,\n   'fake_sampler': False,\n   'framework': 'torch',\n   'eager_tracing': False,\n   'eager_max_retraces': 20,\n   'explore': False,\n   'exploration_config': {'type': 'OrnsteinUhlenbeckNoise',\n    'random_timesteps': 1000,\n    'ou_base_scale': 0.1,\n    'ou_theta': 0.15,\n    'ou_sigma': 0.2,\n    'initial_scale': 1.0,\n    'final_scale': 0.02,\n    'scale_timesteps': 10000},\n   'evaluation_interval': None,\n   'evaluation_duration': 10,\n   'evaluation_duration_unit': 'episodes',\n   'evaluation_parallel_to_training': False,\n   'in_evaluation': False,\n   'evaluation_config': {'explore': False},\n   'evaluation_num_workers': 0,\n   'custom_eval_function': None,\n   'always_attach_evaluation_results': False,\n   'keep_per_episode_custom_metrics': False,\n   'sample_async': False,\n   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n   'observation_filter': 'NoFilter',\n   'synchronize_filters': True,\n   'tf_session_args': {'intra_op_parallelism_threads': 2,\n    'inter_op_parallelism_threads': 2,\n    'gpu_options': {'allow_growth': True},\n    'log_device_placement': False,\n    'device_count': {'CPU': 1},\n    'allow_soft_placement': True},\n   'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n    'inter_op_parallelism_threads': 8},\n   'compress_observations': False,\n   'metrics_episode_collection_timeout_s': 180,\n   'metrics_num_episodes_for_smoothing': 100,\n   'min_time_s_per_reporting': 1,\n   'min_train_timesteps_per_reporting': None,\n   'min_sample_timesteps_per_reporting': 1000,\n   'seed': None,\n   'extra_python_environs_for_driver': {},\n   'extra_python_environs_for_worker': {},\n   'num_gpus': 1,\n   '_fake_gpus': False,\n   'num_cpus_per_worker': 1,\n   'num_gpus_per_worker': 0,\n   'custom_resources_per_worker': {},\n   'num_cpus_for_driver': 1,\n   'placement_strategy': 'PACK',\n   'input': 'sampler',\n   'input_config': {},\n   'actions_in_input_normalized': False,\n   'input_evaluation': ['is', 'wis'],\n   'postprocess_inputs': False,\n   'shuffle_buffer_size': 0,\n   'output': None,\n   'output_config': {},\n   'output_compress_columns': ['obs', 'new_obs'],\n   'output_max_file_size': 67108864,\n   'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.policy_template.DDPGTorchPolicy'>, observation_space=Box([ -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.\n       -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.   0.   0.   0.   0.   0.   0.\n        0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n        0.   0. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300.\n      300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 400. 400. 400. 400.\n      400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400.\n      400. 400. 400. 400.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.\n       -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.], [  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n        0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   10.   10.\n       10.   10.   10.   10.   10.   10.   10.   10.   10.   10.   10.   10.\n       10.   10.   10.   10.   10.   10.   10.   10.  300.  300.  300.  300.\n      300.  300.  300.  300.  300.  300.  300.  300.  300.  300.  300.  300.\n      300.  300.  300.  300.  300.  300.  400.  400.  400.  400.  400.  400.\n      400.  400.  400.  400.  400.  400.  400.  400.  400.  400.  400.  400.\n      400.  400.  400.  400.    1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5\n        1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5\n        1.5   1.5], (110,), float64), action_space=Box([ -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.\n       -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5. 300. 300. 300. 300. 300. 300.\n      300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300.\n      300. 300.], [ 10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.\n       10.  10.  10.  10.  10.  10.  10.  10. 400. 400. 400. 400. 400. 400.\n      400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400.\n      400. 400.], (44,), float64), config={})},\n    'policy_map_capacity': 100,\n    'policy_map_cache': None,\n    'policy_mapping_fn': None,\n    'policies_to_train': None,\n    'observation_fn': None,\n    'replay_mode': 'independent',\n    'count_steps_by': 'env_steps'},\n   'logger_config': None,\n   '_tf_policy_handles_more_than_one_loss': False,\n   '_disable_preprocessor_api': False,\n   '_disable_action_flattening': False,\n   '_disable_execution_plan_api': False,\n   'disable_env_checking': False,\n   'simple_optimizer': False,\n   'monitor': -1,\n   'evaluation_num_episodes': -1,\n   'metrics_smoothing_episodes': -1,\n   'timesteps_per_iteration': 1000,\n   'min_iter_time_s': -1,\n   'collect_metrics_timeout': -1,\n   'twin_q': False,\n   'policy_delay': 1,\n   'smooth_target_policy': False,\n   'target_noise': 0.2,\n   'target_noise_clip': 0.5,\n   'use_state_preprocessor': False,\n   'actor_hiddens': [400, 300],\n   'actor_hidden_activation': 'relu',\n   'critic_hiddens': [400, 300],\n   'critic_hidden_activation': 'relu',\n   'n_step': 1,\n   'buffer_size': -1,\n   'replay_buffer_config': {'type': 'MultiAgentReplayBuffer',\n    'capacity': 50000},\n   'store_buffer_in_checkpoints': False,\n   'prioritized_replay': True,\n   'prioritized_replay_alpha': 0.6,\n   'prioritized_replay_beta': 0.4,\n   'prioritized_replay_beta_annealing_timesteps': 20000,\n   'final_prioritized_replay_beta': 0.4,\n   'prioritized_replay_eps': 1e-06,\n   'training_intensity': None,\n   'critic_lr': 0.001,\n   'actor_lr': 0.001,\n   'target_network_update_freq': 0,\n   'tau': 0.002,\n   'use_huber': False,\n   'huber_threshold': 1.0,\n   'l2_reg': 1e-06,\n   'grad_clip': None,\n   'learning_starts': 1500,\n   'worker_side_prioritization': False},\n  'evaluation_num_workers': 0,\n  'custom_eval_function': None,\n  'always_attach_evaluation_results': False,\n  'keep_per_episode_custom_metrics': False,\n  'sample_async': False,\n  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n  'observation_filter': 'NoFilter',\n  'synchronize_filters': True,\n  'tf_session_args': {'intra_op_parallelism_threads': 2,\n   'inter_op_parallelism_threads': 2,\n   'gpu_options': {'allow_growth': True},\n   'log_device_placement': False,\n   'device_count': {'CPU': 1},\n   'allow_soft_placement': True},\n  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n   'inter_op_parallelism_threads': 8},\n  'compress_observations': False,\n  'metrics_episode_collection_timeout_s': 180,\n  'metrics_num_episodes_for_smoothing': 100,\n  'min_time_s_per_reporting': 1,\n  'min_train_timesteps_per_reporting': None,\n  'min_sample_timesteps_per_reporting': 1000,\n  'seed': None,\n  'extra_python_environs_for_driver': {},\n  'extra_python_environs_for_worker': {},\n  'num_gpus': 1,\n  '_fake_gpus': False,\n  'num_cpus_per_worker': 1,\n  'num_gpus_per_worker': 0,\n  'custom_resources_per_worker': {},\n  'num_cpus_for_driver': 1,\n  'placement_strategy': 'PACK',\n  'input': 'sampler',\n  'input_config': {},\n  'actions_in_input_normalized': False,\n  'input_evaluation': ['is', 'wis'],\n  'postprocess_inputs': False,\n  'shuffle_buffer_size': 0,\n  'output': None,\n  'output_config': {},\n  'output_compress_columns': ['obs', 'new_obs'],\n  'output_max_file_size': 67108864,\n  'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.policy_template.DDPGTorchPolicy'>, observation_space=Box([ -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.\n      -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.   0.   0.   0.   0.   0.   0.\n       0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n       0.   0. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300.\n     300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 400. 400. 400. 400.\n     400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400.\n     400. 400. 400. 400.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.\n      -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.], [  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n       0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   10.   10.\n      10.   10.   10.   10.   10.   10.   10.   10.   10.   10.   10.   10.\n      10.   10.   10.   10.   10.   10.   10.   10.  300.  300.  300.  300.\n     300.  300.  300.  300.  300.  300.  300.  300.  300.  300.  300.  300.\n     300.  300.  300.  300.  300.  300.  400.  400.  400.  400.  400.  400.\n     400.  400.  400.  400.  400.  400.  400.  400.  400.  400.  400.  400.\n     400.  400.  400.  400.    1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5\n       1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5\n       1.5   1.5], (110,), float64), action_space=Box([ -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.\n      -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5. 300. 300. 300. 300. 300. 300.\n     300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300.\n     300. 300.], [ 10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.\n      10.  10.  10.  10.  10.  10.  10.  10. 400. 400. 400. 400. 400. 400.\n     400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400.\n     400. 400.], (44,), float64), config={})},\n   'policy_map_capacity': 100,\n   'policy_map_cache': None,\n   'policy_mapping_fn': None,\n   'policies_to_train': None,\n   'observation_fn': None,\n   'replay_mode': 'independent',\n   'count_steps_by': 'env_steps'},\n  'logger_config': None,\n  '_tf_policy_handles_more_than_one_loss': False,\n  '_disable_preprocessor_api': False,\n  '_disable_action_flattening': False,\n  '_disable_execution_plan_api': False,\n  'disable_env_checking': False,\n  'simple_optimizer': False,\n  'monitor': -1,\n  'evaluation_num_episodes': -1,\n  'metrics_smoothing_episodes': -1,\n  'timesteps_per_iteration': 1000,\n  'min_iter_time_s': -1,\n  'collect_metrics_timeout': -1,\n  'twin_q': False,\n  'policy_delay': 1,\n  'smooth_target_policy': False,\n  'target_noise': 0.2,\n  'target_noise_clip': 0.5,\n  'use_state_preprocessor': False,\n  'actor_hiddens': [400, 300],\n  'actor_hidden_activation': 'relu',\n  'critic_hiddens': [400, 300],\n  'critic_hidden_activation': 'relu',\n  'n_step': 1,\n  'buffer_size': -1,\n  'replay_buffer_config': {'type': 'MultiAgentReplayBuffer',\n   'capacity': 50000},\n  'store_buffer_in_checkpoints': False,\n  'prioritized_replay': True,\n  'prioritized_replay_alpha': 0.6,\n  'prioritized_replay_beta': 0.4,\n  'prioritized_replay_beta_annealing_timesteps': 20000,\n  'final_prioritized_replay_beta': 0.4,\n  'prioritized_replay_eps': 1e-06,\n  'training_intensity': None,\n  'critic_lr': 0.001,\n  'actor_lr': 0.001,\n  'target_network_update_freq': 0,\n  'tau': 0.002,\n  'use_huber': False,\n  'huber_threshold': 1.0,\n  'l2_reg': 1e-06,\n  'grad_clip': None,\n  'learning_starts': 1500,\n  'worker_side_prioritization': False},\n 'time_since_restore': 80.10315036773682,\n 'timesteps_since_restore': 512,\n 'iterations_since_restore': 2,\n 'warmup_time': 148.53814721107483,\n 'perf': {'cpu_util_percent': 14.966806722689073,\n  'ram_util_percent': 71.75798319327728}}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{'episode_reward_max': -7050.0,\n 'episode_reward_min': -8315.808311700821,\n 'episode_reward_mean': -7467.831740522304,\n 'episode_len_mean': 47.0,\n 'episode_media': {},\n 'episodes_this_iter': 21,\n 'policy_reward_min': {},\n 'policy_reward_max': {},\n 'policy_reward_mean': {},\n 'custom_metrics': {},\n 'hist_stats': {'episode_reward': [-8027.586515247822,\n   -8315.808311700821,\n   -8189.849503338337,\n   -8240.037979483604,\n   -8007.36781001091,\n   -8142.452227175236,\n   -8166.524214744568,\n   -8141.8783454597,\n   -8054.372584074736,\n   -7956.5983410179615,\n   -8257.948513627052,\n   -8184.76120531559,\n   -8210.522580891848,\n   -8251.272272616625,\n   -8127.221717238426,\n   -8222.58759751916,\n   -8066.885175853968,\n   -8105.836598575115,\n   -8123.126549571753,\n   -8042.244348675013,\n   -8146.273148953915,\n   -7817.523749470711,\n   -7755.0,\n   -7755.0,\n   -7755.0,\n   -7755.0,\n   -7755.0,\n   -7755.0,\n   -7755.0,\n   -7755.869508087635,\n   -7755.0,\n   -7725.0,\n   -7250.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0,\n   -7050.0],\n  'episode_lengths': [47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47,\n   47]},\n 'sampler_perf': {'mean_raw_obs_processing_ms': 2.691030005955519,\n  'mean_inference_ms': 1.378908350954389,\n  'mean_action_processing_ms': 0.1587899222666353,\n  'mean_env_wait_ms': 1.407405141344922,\n  'mean_env_render_ms': 0.0},\n 'off_policy_estimator': {},\n 'num_healthy_workers': 1,\n 'timesteps_total': 3500,\n 'timesteps_this_iter': 256,\n 'agent_timesteps_total': 3500,\n 'timers': {'load_time_ms': 2.386,\n  'load_throughput': 107290.496,\n  'learn_time_ms': 28.416,\n  'learn_throughput': 9009.11,\n  'update_time_ms': 4.791},\n 'info': {'learner': {'default_policy': {'custom_metrics': {},\n    'learner_stats': {'actor_loss': 894.0194702148438,\n     'critic_loss': 1608.2866186506044,\n     'mean_q': -908.3112182617188,\n     'max_q': -361.6376953125,\n     'min_q': -1020.9227294921875},\n    'model': {},\n    'td_error': array([-5.65612793e-01,  4.94620361e+01, -2.05267334e+01, -2.42731323e+01,\n           -1.16554688e+02, -5.17879028e+01, -2.88220215e+00,  4.54895020e+00,\n           -4.40063477e+00,  9.39489136e+01,  3.97283325e+01,  2.06671143e+00,\n           -4.35496216e+01, -3.06600342e+01, -2.73228760e+01, -5.01770020e+00,\n            1.45604248e+01,  2.25335693e+00,  2.71252441e+00, -5.05366821e+01,\n            3.81332397e+01, -5.28728638e+01,  2.36407471e+00, -2.39330750e+02,\n           -1.90525513e+01, -6.40484619e+00, -1.14247437e+01, -4.17980957e+00,\n            2.42785645e+00, -2.02358093e+02,  2.84771729e+00,  4.46759033e+00,\n           -1.74371948e+01,  5.57971191e+00,  2.49804688e+01,  9.83739624e+01,\n            3.04718018e+00, -1.54021606e+01,  4.24181519e+01, -4.06201172e+00,\n            4.12731934e+00,  5.57788086e+00,  1.88446045e+00,  2.98329590e+02,\n           -3.06600342e+01,  2.71051025e+00,  6.14605713e+00,  1.47486023e+02,\n            5.91101685e+01,  4.70202637e+00,  5.42083740e+00,  4.78344727e+00,\n           -1.32422485e+01,  1.01575745e+02,  3.87225952e+01,  1.31945190e+01,\n           -6.89475708e+01, -5.71716309e-01, -2.24037476e+01, -7.06340942e+01,\n            5.91101685e+01,  4.00607300e+01, -1.03381531e+02,  4.10638428e+00,\n            1.57446289e+00,  4.33654785e+00, -9.28344727e-01, -2.35225220e+01,\n           -3.43066406e+00, -1.09708252e+01,  2.03771973e+01, -3.89480591e+01,\n           -8.00323486e+00,  4.39578247e+01, -7.79724121e-01,  6.82562256e+00,\n           -1.11364746e+00,  1.29981079e+01,  4.23321533e+00,  5.01745605e+00,\n           -8.15631104e+01,  5.63824463e+00, -7.41454468e+01, -9.77783203e+00,\n            1.35670166e+01,  3.55798340e+00, -2.22584717e+02,  5.39855957e+00,\n            1.47320557e+00, -6.39439087e+01,  1.62701416e+00,  2.31646729e+00,\n            4.99554443e+00, -2.41809082e+00,  8.09509277e-01,  2.83519287e+01,\n           -4.47468262e+01,  2.80291748e+01, -1.11364746e+00,  4.66430664e+00,\n            9.10706787e+01, -4.61468506e+00, -2.18161652e+02, -3.34265137e+00,\n           -4.83312988e+00, -1.08999634e+01,  3.09197998e+00, -5.73901367e+00,\n           -9.60314941e+00,  9.38629150e+00,  4.84518433e+01, -2.92515259e+01,\n           -7.07697754e+01, -4.25945435e+01, -1.17793579e+01, -6.76777344e+01,\n            1.58484497e+01,  1.86575317e+01, -2.48229980e-01, -9.57183838e+00,\n            3.43566284e+01, -1.97155762e+00, -8.29669189e+00, -2.12653442e+02,\n            4.32460938e+01,  1.37749084e+02, -8.27416992e+00,  2.71502686e+00,\n           -4.89654541e+00,  4.76815796e+01,  2.71051025e+00, -3.09091187e+01,\n           -8.24862671e+01,  1.58800659e+01,  5.01745605e+00, -1.00919800e+01,\n           -2.82429810e+01, -7.29937744e+00,  5.57514038e+01, -4.71270752e+00,\n           -1.82736084e+02,  1.42470093e+01, -1.13531433e+02, -1.07609863e+01,\n            6.36114502e+00,  5.73553467e+00, -8.40179443e+00, -3.15047607e+01,\n           -6.13580933e+01,  2.93082886e+01,  8.55003052e+01, -3.18908081e+01,\n            4.80487671e+01,  4.69427490e+00, -1.53452759e+01, -1.82736084e+02,\n           -7.48098145e+01, -1.21831665e+01, -9.78594971e+00, -5.81604004e-01,\n            1.94724121e+01, -1.29427490e+01,  4.72229004e+00, -3.28621216e+01,\n            2.90008545e+00, -1.62310791e+01,  2.82647095e+01, -3.81976318e+00,\n           -6.93572998e+00,  5.59754028e+01,  6.65069580e+00, -6.42749023e+00,\n            5.20916748e+01, -1.84069824e+00, -1.03225952e+02,  5.04915771e+01,\n            4.15130615e+00,  4.12188721e+00, -1.63360596e+00,  2.50817871e+00,\n           -1.01056519e+01,  2.10938721e+01, -1.68814697e+01, -1.64326782e+01,\n            9.75698853e+01,  1.62701416e+00,  4.54699707e+00, -2.84880371e+01,\n           -9.86749268e+01, -4.20208740e+01, -7.30316162e+00,  2.08905029e+00,\n           -4.78637695e-01, -8.65063477e+00,  9.83739624e+01,  7.47436523e-01,\n            4.44897461e+00,  5.16314087e+01, -1.48530884e+01,  1.79818726e+01,\n           -1.13649902e+01, -2.70574951e+00,  1.58172607e+00, -6.35013428e+01,\n            2.25638428e+01,  4.75415039e+00,  7.50732422e-01, -1.01326294e+01,\n           -6.17675781e-01, -1.58081055e+01,  2.58380127e+00,  5.17730713e+00,\n           -6.16815186e+00, -5.01495972e+01, -6.24230957e+00, -1.82780762e+01,\n            7.09753418e+00, -1.36377563e+01,  4.51849365e+00, -1.87375488e+01,\n           -5.74523926e+00,  9.70402222e+01, -9.54588623e+01, -3.00366821e+01,\n           -7.33123779e+00, -6.63452148e-01, -6.93255615e+00,  5.29077148e+00,\n            2.74634399e+01, -1.37302246e+01, -8.75183105e-01,  3.47418213e+00,\n           -8.59155273e+00, -3.33923340e+00,  2.85375977e+01, -2.11637695e+02,\n           -2.45801392e+01, -5.24472046e+01,  2.35609741e+01,  8.54797363e-01,\n            4.90053101e+01, -9.46417236e+00,  1.48416199e+02, -3.67237549e+01,\n            1.03707520e+02,  4.06793213e+00,  3.61071777e+00,  5.04370117e+01,\n            7.15809937e+01, -2.63472900e+01, -1.69313965e+01, -6.41491699e+01,\n            8.54797363e-01,  4.24560547e+00, -2.07982178e+01, -1.43397827e+01],\n          dtype=float32),\n    'mean_td_error': -5.513251304626465}},\n  'num_steps_sampled': 3500,\n  'num_agent_steps_sampled': 3500,\n  'num_steps_trained': 512256,\n  'num_steps_trained_this_iter': 256,\n  'num_agent_steps_trained': 512256,\n  'last_target_update_ts': 3500,\n  'num_target_updates': 2001},\n 'done': False,\n 'episodes_total': 74,\n 'training_iteration': 3,\n 'trial_id': 'default',\n 'experiment_id': '40b4f75474f74cceb3b69a0d9d0c8c3d',\n 'date': '2022-05-25_13-05-35',\n 'timestamp': 1653476735,\n 'time_this_iter_s': 65.64075684547424,\n 'time_total_s': 145.74390721321106,\n 'pid': 4828,\n 'hostname': 'DESKTOP-M33545T',\n 'node_ip': '127.0.0.1',\n 'config': {'num_workers': 1,\n  'num_envs_per_worker': 1,\n  'create_env_on_driver': False,\n  'rollout_fragment_length': 1,\n  'batch_mode': 'truncate_episodes',\n  'gamma': 0.99,\n  'lr': 0.0001,\n  'train_batch_size': 256,\n  'model': {'_use_default_native_models': False,\n   '_disable_preprocessor_api': False,\n   '_disable_action_flattening': False,\n   'fcnet_hiddens': [256, 256],\n   'fcnet_activation': 'tanh',\n   'conv_filters': None,\n   'conv_activation': 'relu',\n   'post_fcnet_hiddens': [],\n   'post_fcnet_activation': 'relu',\n   'free_log_std': False,\n   'no_final_linear': False,\n   'vf_share_layers': True,\n   'use_lstm': False,\n   'max_seq_len': 20,\n   'lstm_cell_size': 256,\n   'lstm_use_prev_action': False,\n   'lstm_use_prev_reward': False,\n   '_time_major': False,\n   'use_attention': False,\n   'attention_num_transformer_units': 1,\n   'attention_dim': 64,\n   'attention_num_heads': 1,\n   'attention_head_dim': 32,\n   'attention_memory_inference': 50,\n   'attention_memory_training': 50,\n   'attention_position_wise_mlp_dim': 32,\n   'attention_init_gru_gate_bias': 2.0,\n   'attention_use_n_prev_actions': 0,\n   'attention_use_n_prev_rewards': 0,\n   'framestack': True,\n   'dim': 84,\n   'grayscale': False,\n   'zero_mean': True,\n   'custom_model': None,\n   'custom_model_config': {},\n   'custom_action_dist': None,\n   'custom_preprocessor': None,\n   'lstm_use_prev_action_reward': -1},\n  'optimizer': {},\n  'horizon': None,\n  'soft_horizon': False,\n  'no_done_at_end': False,\n  'env': 'my_env',\n  'observation_space': None,\n  'action_space': None,\n  'env_config': {},\n  'remote_worker_envs': False,\n  'remote_env_batch_wait_ms': 0,\n  'env_task_fn': None,\n  'render_env': False,\n  'record_env': False,\n  'clip_rewards': None,\n  'normalize_actions': True,\n  'clip_actions': False,\n  'preprocessor_pref': 'deepmind',\n  'log_level': 'WARN',\n  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n  'ignore_worker_failures': False,\n  'log_sys_usage': True,\n  'fake_sampler': False,\n  'framework': 'torch',\n  'eager_tracing': False,\n  'eager_max_retraces': 20,\n  'explore': True,\n  'exploration_config': {'type': 'OrnsteinUhlenbeckNoise',\n   'random_timesteps': 1000,\n   'ou_base_scale': 0.1,\n   'ou_theta': 0.15,\n   'ou_sigma': 0.2,\n   'initial_scale': 1.0,\n   'final_scale': 0.02,\n   'scale_timesteps': 10000},\n  'evaluation_interval': None,\n  'evaluation_duration': 10,\n  'evaluation_duration_unit': 'episodes',\n  'evaluation_parallel_to_training': False,\n  'in_evaluation': False,\n  'evaluation_config': {'num_workers': 1,\n   'num_envs_per_worker': 1,\n   'create_env_on_driver': False,\n   'rollout_fragment_length': 1,\n   'batch_mode': 'truncate_episodes',\n   'gamma': 0.99,\n   'lr': 0.0001,\n   'train_batch_size': 256,\n   'model': {'_use_default_native_models': False,\n    '_disable_preprocessor_api': False,\n    '_disable_action_flattening': False,\n    'fcnet_hiddens': [256, 256],\n    'fcnet_activation': 'tanh',\n    'conv_filters': None,\n    'conv_activation': 'relu',\n    'post_fcnet_hiddens': [],\n    'post_fcnet_activation': 'relu',\n    'free_log_std': False,\n    'no_final_linear': False,\n    'vf_share_layers': True,\n    'use_lstm': False,\n    'max_seq_len': 20,\n    'lstm_cell_size': 256,\n    'lstm_use_prev_action': False,\n    'lstm_use_prev_reward': False,\n    '_time_major': False,\n    'use_attention': False,\n    'attention_num_transformer_units': 1,\n    'attention_dim': 64,\n    'attention_num_heads': 1,\n    'attention_head_dim': 32,\n    'attention_memory_inference': 50,\n    'attention_memory_training': 50,\n    'attention_position_wise_mlp_dim': 32,\n    'attention_init_gru_gate_bias': 2.0,\n    'attention_use_n_prev_actions': 0,\n    'attention_use_n_prev_rewards': 0,\n    'framestack': True,\n    'dim': 84,\n    'grayscale': False,\n    'zero_mean': True,\n    'custom_model': None,\n    'custom_model_config': {},\n    'custom_action_dist': None,\n    'custom_preprocessor': None,\n    'lstm_use_prev_action_reward': -1},\n   'optimizer': {},\n   'horizon': None,\n   'soft_horizon': False,\n   'no_done_at_end': False,\n   'env': 'my_env',\n   'observation_space': None,\n   'action_space': None,\n   'env_config': {},\n   'remote_worker_envs': False,\n   'remote_env_batch_wait_ms': 0,\n   'env_task_fn': None,\n   'render_env': False,\n   'record_env': False,\n   'clip_rewards': None,\n   'normalize_actions': True,\n   'clip_actions': False,\n   'preprocessor_pref': 'deepmind',\n   'log_level': 'WARN',\n   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n   'ignore_worker_failures': False,\n   'log_sys_usage': True,\n   'fake_sampler': False,\n   'framework': 'torch',\n   'eager_tracing': False,\n   'eager_max_retraces': 20,\n   'explore': False,\n   'exploration_config': {'type': 'OrnsteinUhlenbeckNoise',\n    'random_timesteps': 1000,\n    'ou_base_scale': 0.1,\n    'ou_theta': 0.15,\n    'ou_sigma': 0.2,\n    'initial_scale': 1.0,\n    'final_scale': 0.02,\n    'scale_timesteps': 10000},\n   'evaluation_interval': None,\n   'evaluation_duration': 10,\n   'evaluation_duration_unit': 'episodes',\n   'evaluation_parallel_to_training': False,\n   'in_evaluation': False,\n   'evaluation_config': {'explore': False},\n   'evaluation_num_workers': 0,\n   'custom_eval_function': None,\n   'always_attach_evaluation_results': False,\n   'keep_per_episode_custom_metrics': False,\n   'sample_async': False,\n   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n   'observation_filter': 'NoFilter',\n   'synchronize_filters': True,\n   'tf_session_args': {'intra_op_parallelism_threads': 2,\n    'inter_op_parallelism_threads': 2,\n    'gpu_options': {'allow_growth': True},\n    'log_device_placement': False,\n    'device_count': {'CPU': 1},\n    'allow_soft_placement': True},\n   'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n    'inter_op_parallelism_threads': 8},\n   'compress_observations': False,\n   'metrics_episode_collection_timeout_s': 180,\n   'metrics_num_episodes_for_smoothing': 100,\n   'min_time_s_per_reporting': 1,\n   'min_train_timesteps_per_reporting': None,\n   'min_sample_timesteps_per_reporting': 1000,\n   'seed': None,\n   'extra_python_environs_for_driver': {},\n   'extra_python_environs_for_worker': {},\n   'num_gpus': 1,\n   '_fake_gpus': False,\n   'num_cpus_per_worker': 1,\n   'num_gpus_per_worker': 0,\n   'custom_resources_per_worker': {},\n   'num_cpus_for_driver': 1,\n   'placement_strategy': 'PACK',\n   'input': 'sampler',\n   'input_config': {},\n   'actions_in_input_normalized': False,\n   'input_evaluation': ['is', 'wis'],\n   'postprocess_inputs': False,\n   'shuffle_buffer_size': 0,\n   'output': None,\n   'output_config': {},\n   'output_compress_columns': ['obs', 'new_obs'],\n   'output_max_file_size': 67108864,\n   'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.policy_template.DDPGTorchPolicy'>, observation_space=Box([ -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.\n       -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.   0.   0.   0.   0.   0.   0.\n        0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n        0.   0. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300.\n      300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 400. 400. 400. 400.\n      400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400.\n      400. 400. 400. 400.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.\n       -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.], [  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n        0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   10.   10.\n       10.   10.   10.   10.   10.   10.   10.   10.   10.   10.   10.   10.\n       10.   10.   10.   10.   10.   10.   10.   10.  300.  300.  300.  300.\n      300.  300.  300.  300.  300.  300.  300.  300.  300.  300.  300.  300.\n      300.  300.  300.  300.  300.  300.  400.  400.  400.  400.  400.  400.\n      400.  400.  400.  400.  400.  400.  400.  400.  400.  400.  400.  400.\n      400.  400.  400.  400.    1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5\n        1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5\n        1.5   1.5], (110,), float64), action_space=Box([ -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.\n       -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5. 300. 300. 300. 300. 300. 300.\n      300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300.\n      300. 300.], [ 10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.\n       10.  10.  10.  10.  10.  10.  10.  10. 400. 400. 400. 400. 400. 400.\n      400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400.\n      400. 400.], (44,), float64), config={})},\n    'policy_map_capacity': 100,\n    'policy_map_cache': None,\n    'policy_mapping_fn': None,\n    'policies_to_train': None,\n    'observation_fn': None,\n    'replay_mode': 'independent',\n    'count_steps_by': 'env_steps'},\n   'logger_config': None,\n   '_tf_policy_handles_more_than_one_loss': False,\n   '_disable_preprocessor_api': False,\n   '_disable_action_flattening': False,\n   '_disable_execution_plan_api': False,\n   'disable_env_checking': False,\n   'simple_optimizer': False,\n   'monitor': -1,\n   'evaluation_num_episodes': -1,\n   'metrics_smoothing_episodes': -1,\n   'timesteps_per_iteration': 1000,\n   'min_iter_time_s': -1,\n   'collect_metrics_timeout': -1,\n   'twin_q': False,\n   'policy_delay': 1,\n   'smooth_target_policy': False,\n   'target_noise': 0.2,\n   'target_noise_clip': 0.5,\n   'use_state_preprocessor': False,\n   'actor_hiddens': [400, 300],\n   'actor_hidden_activation': 'relu',\n   'critic_hiddens': [400, 300],\n   'critic_hidden_activation': 'relu',\n   'n_step': 1,\n   'buffer_size': -1,\n   'replay_buffer_config': {'type': 'MultiAgentReplayBuffer',\n    'capacity': 50000},\n   'store_buffer_in_checkpoints': False,\n   'prioritized_replay': True,\n   'prioritized_replay_alpha': 0.6,\n   'prioritized_replay_beta': 0.4,\n   'prioritized_replay_beta_annealing_timesteps': 20000,\n   'final_prioritized_replay_beta': 0.4,\n   'prioritized_replay_eps': 1e-06,\n   'training_intensity': None,\n   'critic_lr': 0.001,\n   'actor_lr': 0.001,\n   'target_network_update_freq': 0,\n   'tau': 0.002,\n   'use_huber': False,\n   'huber_threshold': 1.0,\n   'l2_reg': 1e-06,\n   'grad_clip': None,\n   'learning_starts': 1500,\n   'worker_side_prioritization': False},\n  'evaluation_num_workers': 0,\n  'custom_eval_function': None,\n  'always_attach_evaluation_results': False,\n  'keep_per_episode_custom_metrics': False,\n  'sample_async': False,\n  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n  'observation_filter': 'NoFilter',\n  'synchronize_filters': True,\n  'tf_session_args': {'intra_op_parallelism_threads': 2,\n   'inter_op_parallelism_threads': 2,\n   'gpu_options': {'allow_growth': True},\n   'log_device_placement': False,\n   'device_count': {'CPU': 1},\n   'allow_soft_placement': True},\n  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n   'inter_op_parallelism_threads': 8},\n  'compress_observations': False,\n  'metrics_episode_collection_timeout_s': 180,\n  'metrics_num_episodes_for_smoothing': 100,\n  'min_time_s_per_reporting': 1,\n  'min_train_timesteps_per_reporting': None,\n  'min_sample_timesteps_per_reporting': 1000,\n  'seed': None,\n  'extra_python_environs_for_driver': {},\n  'extra_python_environs_for_worker': {},\n  'num_gpus': 1,\n  '_fake_gpus': False,\n  'num_cpus_per_worker': 1,\n  'num_gpus_per_worker': 0,\n  'custom_resources_per_worker': {},\n  'num_cpus_for_driver': 1,\n  'placement_strategy': 'PACK',\n  'input': 'sampler',\n  'input_config': {},\n  'actions_in_input_normalized': False,\n  'input_evaluation': ['is', 'wis'],\n  'postprocess_inputs': False,\n  'shuffle_buffer_size': 0,\n  'output': None,\n  'output_config': {},\n  'output_compress_columns': ['obs', 'new_obs'],\n  'output_max_file_size': 67108864,\n  'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.policy_template.DDPGTorchPolicy'>, observation_space=Box([ -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.\n      -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.   0.   0.   0.   0.   0.   0.\n       0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n       0.   0. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300.\n     300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 400. 400. 400. 400.\n     400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400.\n     400. 400. 400. 400.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.\n      -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.], [  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n       0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   10.   10.\n      10.   10.   10.   10.   10.   10.   10.   10.   10.   10.   10.   10.\n      10.   10.   10.   10.   10.   10.   10.   10.  300.  300.  300.  300.\n     300.  300.  300.  300.  300.  300.  300.  300.  300.  300.  300.  300.\n     300.  300.  300.  300.  300.  300.  400.  400.  400.  400.  400.  400.\n     400.  400.  400.  400.  400.  400.  400.  400.  400.  400.  400.  400.\n     400.  400.  400.  400.    1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5\n       1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5\n       1.5   1.5], (110,), float64), action_space=Box([ -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.\n      -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5. 300. 300. 300. 300. 300. 300.\n     300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300. 300.\n     300. 300.], [ 10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.  10.\n      10.  10.  10.  10.  10.  10.  10.  10. 400. 400. 400. 400. 400. 400.\n     400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400. 400.\n     400. 400.], (44,), float64), config={})},\n   'policy_map_capacity': 100,\n   'policy_map_cache': None,\n   'policy_mapping_fn': None,\n   'policies_to_train': None,\n   'observation_fn': None,\n   'replay_mode': 'independent',\n   'count_steps_by': 'env_steps'},\n  'logger_config': None,\n  '_tf_policy_handles_more_than_one_loss': False,\n  '_disable_preprocessor_api': False,\n  '_disable_action_flattening': False,\n  '_disable_execution_plan_api': False,\n  'disable_env_checking': False,\n  'simple_optimizer': False,\n  'monitor': -1,\n  'evaluation_num_episodes': -1,\n  'metrics_smoothing_episodes': -1,\n  'timesteps_per_iteration': 1000,\n  'min_iter_time_s': -1,\n  'collect_metrics_timeout': -1,\n  'twin_q': False,\n  'policy_delay': 1,\n  'smooth_target_policy': False,\n  'target_noise': 0.2,\n  'target_noise_clip': 0.5,\n  'use_state_preprocessor': False,\n  'actor_hiddens': [400, 300],\n  'actor_hidden_activation': 'relu',\n  'critic_hiddens': [400, 300],\n  'critic_hidden_activation': 'relu',\n  'n_step': 1,\n  'buffer_size': -1,\n  'replay_buffer_config': {'type': 'MultiAgentReplayBuffer',\n   'capacity': 50000},\n  'store_buffer_in_checkpoints': False,\n  'prioritized_replay': True,\n  'prioritized_replay_alpha': 0.6,\n  'prioritized_replay_beta': 0.4,\n  'prioritized_replay_beta_annealing_timesteps': 20000,\n  'final_prioritized_replay_beta': 0.4,\n  'prioritized_replay_eps': 1e-06,\n  'training_intensity': None,\n  'critic_lr': 0.001,\n  'actor_lr': 0.001,\n  'target_network_update_freq': 0,\n  'tau': 0.002,\n  'use_huber': False,\n  'huber_threshold': 1.0,\n  'l2_reg': 1e-06,\n  'grad_clip': None,\n  'learning_starts': 1500,\n  'worker_side_prioritization': False},\n 'time_since_restore': 145.74390721321106,\n 'timesteps_since_restore': 768,\n 'iterations_since_restore': 3,\n 'warmup_time': 148.53814721107483,\n 'perf': {'cpu_util_percent': 15.205882352941176,\n  'ram_util_percent': 72.01680672268908}}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d7053eaf1f44e4ba09689f8d46ffe60bb595916505f14727b0e14a5d0bba04d"
  },
  "kernelspec": {
   "name": "rl-evcp",
   "language": "python",
   "display_name": "RL-EVCP"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}