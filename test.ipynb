{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.environments.create_env import create_env\n",
    "from src.samplers.load_samplers import load_samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading funky environment (this may take a hot minute)\n",
      "Size of State Space ->  110\n",
      "Size of Action Space ->  44\n",
      "Max Value of Action ->  10.0\n",
      "Min Value of Action ->  -5.0\n"
     ]
    }
   ],
   "source": [
    "# path_to_this_notebook = os.path.abspath('.')\n",
    "# path_to_project = path_to_this_notebook[:path_to_this_notebook.find('src')]\n",
    "# sys.path.append(path_to_project)\n",
    "config = {'path_to_data':   './data/',\n",
    "          't0_hr': 6.,  # When the episode start (default value 6AM)\n",
    "          'dt_min': 30,  # Timestep size\n",
    "          'ev_dt_min': 60,  # Timestep size for EV arrivals\n",
    "          'ev_sampling_dt_min': 60,  # How EV sessions are sampled from the data\n",
    "          'apply_gaussian_noise': False,  # Make data noisy\n",
    "          'ev_utility_coef_mean': 1,  # Mean value of the utility coefficient for the EVs\n",
    "          'ev_utility_coef_scale': 0.13,  # STD of the utility coefficient for the EVs\n",
    "          'days_per_month_train': 20,  # Days per month for training\n",
    "          'ev_session_months_train': ['01', '02', '03', '04', '06', '07', '08', '09', '10', '11', ],\n",
    "          # Months to sample EV sessions for training\n",
    "          'grid_to_use': 'ieee16',  # What grid topology to use. Now supports only IEEE16.\n",
    "          'ev_session_months_test': ['05', '12'],  # Months to sample EV sessions for test\n",
    "          'n_ps_pvs': 4,  # Amount of solar panels that use PecanStreet data\n",
    "          'n_canopy_pvs': 0,  # Amount of solar panels that use canopy data\n",
    "          'canopy_pv_rated_power': 250,  # Rated power of these panels\n",
    "          'n_loads': 0,  # Amount of inflexible loads\n",
    "          'n_feeders': 1,  # Amount of feeders\n",
    "          'n_ev_chargers': 4,  # Amount of EV chargers\n",
    "\n",
    "          'ps_pvs_rated_power': 4,  # Rated power of these panels\n",
    "          'avg_evs_per_day': 3.5,  # Scaling of the EV arrival rate\n",
    "          'feeder_p_min': -5,  # Capacity of the feeders\n",
    "          'g': 4,  # Conductance of each line\n",
    "          'i_max': 25,  # Capacity of each line\n",
    "          }\n",
    "\n",
    "\n",
    "def env_creator(env_config):\n",
    "    # Preload samplers, it is necessary to avoid re-loading data each time env is created\n",
    "    (ps_samplers_dict, ps_metadata, canopy_sampler, canopy_metadata,\n",
    "     price_sampler, price_metadata, ev_sampler, elaadnl_metadata) = load_samplers(env_config)\n",
    "\n",
    "    return create_env(\n",
    "        env_config,\n",
    "        ps_samplers_dict,\n",
    "        ps_metadata,\n",
    "        canopy_sampler,\n",
    "        canopy_metadata,\n",
    "        price_sampler,\n",
    "        price_metadata,\n",
    "        ev_sampler,\n",
    "        elaadnl_metadata\n",
    "    )  # return an env instance\n",
    "\n",
    "print('Loading funky environment (this may take a hot minute)')\n",
    "env = env_creator(config)\n",
    "\n",
    "# problem = \"Pendulum-v1\"\n",
    "# env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0/46 * Avg Reward is ==> 108.45322828872101\n",
      "Episode * 1/46 * Avg Reward is ==> 170.19761499345483\n",
      "Episode * 2/46 * Avg Reward is ==> 179.89259845957795\n",
      "Episode * 3/46 * Avg Reward is ==> 179.00176603283467\n",
      "Episode * 4/46 * Avg Reward is ==> 201.56145939920413\n",
      "Episode * 5/46 * Avg Reward is ==> 196.06293000710173\n",
      "Episode * 6/46 * Avg Reward is ==> 207.55936180868997\n",
      "Episode * 7/46 * Avg Reward is ==> 216.83330801463964\n",
      "Episode * 8/46 * Avg Reward is ==> 221.05798648450363\n",
      "Episode * 9/46 * Avg Reward is ==> 216.41997585458066\n",
      "Episode * 10/46 * Avg Reward is ==> 215.91023417417182\n",
      "Episode * 11/46 * Avg Reward is ==> 213.83105931942916\n",
      "Episode * 12/46 * Avg Reward is ==> 222.882429464069\n",
      "Episode * 13/46 * Avg Reward is ==> 223.24285898823715\n",
      "Episode * 14/46 * Avg Reward is ==> 221.29446191089846\n",
      "Episode * 15/46 * Avg Reward is ==> 215.90785794321047\n",
      "Episode * 16/46 * Avg Reward is ==> 212.55851883026966\n",
      "Episode * 17/46 * Avg Reward is ==> 212.4278756894351\n",
      "Episode * 18/46 * Avg Reward is ==> 215.05120147580223\n",
      "Episode * 19/46 * Avg Reward is ==> 216.48501177715087\n",
      "Episode * 20/46 * Avg Reward is ==> 213.59125266343355\n",
      "Episode * 21/46 * Avg Reward is ==> 209.75153881984488\n",
      "Episode * 22/46 * Avg Reward is ==> 204.94072199523046\n",
      "Episode * 23/46 * Avg Reward is ==> 205.0157885519587\n",
      "Episode * 24/46 * Avg Reward is ==> 207.41432138157987\n",
      "Episode * 25/46 * Avg Reward is ==> 207.19078285687678\n",
      "Episode * 26/46 * Avg Reward is ==> 208.25687662217447\n",
      "Episode * 27/46 * Avg Reward is ==> 204.87486842520724\n",
      "Episode * 28/46 * Avg Reward is ==> 205.8052500698671\n",
      "Episode * 29/46 * Avg Reward is ==> 205.28025131055296\n",
      "Episode * 30/46 * Avg Reward is ==> 206.41640996230475\n",
      "Episode * 31/46 * Avg Reward is ==> 206.66191758329563\n",
      "Episode * 32/46 * Avg Reward is ==> 205.3316972415047\n",
      "Episode * 33/46 * Avg Reward is ==> 206.93185659988032\n",
      "Episode * 34/46 * Avg Reward is ==> 204.47709581129712\n",
      "Episode * 35/46 * Avg Reward is ==> 205.94113470335856\n",
      "Episode * 36/46 * Avg Reward is ==> 205.62465895658454\n",
      "Episode * 37/46 * Avg Reward is ==> 207.162671277008\n",
      "Episode * 38/46 * Avg Reward is ==> 207.73114115060005\n",
      "Episode * 39/46 * Avg Reward is ==> 208.44429768760892\n",
      "Episode * 40/46 * Avg Reward is ==> 209.85726600284562\n",
      "Episode * 41/46 * Avg Reward is ==> 213.55344935882948\n",
      "Episode * 42/46 * Avg Reward is ==> 212.4647320128316\n",
      "Episode * 43/46 * Avg Reward is ==> 213.91040367686242\n",
      "Episode * 44/46 * Avg Reward is ==> 212.3857410976912\n",
      "Episode * 45/46 * Avg Reward is ==> 213.88291828801113\n",
      "Episode * 46/46 * Avg Reward is ==> 211.33322046807407\n",
      "Episode * 47/46 * Avg Reward is ==> 207.817389527489\n",
      "Episode * 48/46 * Avg Reward is ==> 206.45913340344333\n",
      "Episode * 49/46 * Avg Reward is ==> 206.40460381846077\n",
      "Episode * 50/46 * Avg Reward is ==> 203.0119148577742\n",
      "Episode * 51/46 * Avg Reward is ==> 201.5104955043792\n",
      "Episode * 52/46 * Avg Reward is ==> 196.68702147678928\n",
      "Episode * 53/46 * Avg Reward is ==> 195.269753963243\n",
      "Episode * 54/46 * Avg Reward is ==> 195.5018477213697\n",
      "Episode * 55/46 * Avg Reward is ==> 193.53472583760214\n",
      "Episode * 56/46 * Avg Reward is ==> 194.04017718623976\n",
      "Episode * 57/46 * Avg Reward is ==> 192.53311991581498\n",
      "Episode * 58/46 * Avg Reward is ==> 190.6626133574156\n",
      "Episode * 59/46 * Avg Reward is ==> 188.0525019337744\n",
      "Episode * 60/46 * Avg Reward is ==> 192.33082654838995\n",
      "Episode * 61/46 * Avg Reward is ==> 192.73235976570533\n",
      "Episode * 62/46 * Avg Reward is ==> 193.69775805363594\n",
      "Episode * 63/46 * Avg Reward is ==> 191.88973139664148\n",
      "Episode * 64/46 * Avg Reward is ==> 189.21379231444183\n",
      "Episode * 65/46 * Avg Reward is ==> 190.29033969625593\n",
      "Episode * 66/46 * Avg Reward is ==> 188.2894206778805\n",
      "Episode * 67/46 * Avg Reward is ==> 190.06108953563836\n",
      "Episode * 68/46 * Avg Reward is ==> 186.1056449080614\n",
      "Episode * 69/46 * Avg Reward is ==> 183.6192024259395\n",
      "Episode * 70/46 * Avg Reward is ==> 179.98174877357934\n",
      "Episode * 71/46 * Avg Reward is ==> 179.7270218160474\n",
      "Episode * 72/46 * Avg Reward is ==> 180.09512202630043\n",
      "Episode * 73/46 * Avg Reward is ==> 178.55617147477747\n",
      "Episode * 74/46 * Avg Reward is ==> 181.06553172431677\n",
      "Episode * 75/46 * Avg Reward is ==> 179.5601644938788\n",
      "Episode * 76/46 * Avg Reward is ==> 179.77487401353724\n",
      "Episode * 77/46 * Avg Reward is ==> 178.72056698274613\n",
      "Episode * 78/46 * Avg Reward is ==> 177.70637841021664\n",
      "Episode * 79/46 * Avg Reward is ==> 175.35127927471174\n",
      "Episode * 80/46 * Avg Reward is ==> 179.74354205621833\n",
      "Episode * 81/46 * Avg Reward is ==> 174.6016655204488\n",
      "Episode * 82/46 * Avg Reward is ==> 174.20318623589262\n",
      "Episode * 83/46 * Avg Reward is ==> 173.8351402506122\n",
      "Episode * 84/46 * Avg Reward is ==> 173.5553601854391\n",
      "Episode * 85/46 * Avg Reward is ==> 171.62624767902753\n",
      "Episode * 86/46 * Avg Reward is ==> 173.20341259080115\n",
      "Episode * 87/46 * Avg Reward is ==> 173.2802814199922\n",
      "Episode * 88/46 * Avg Reward is ==> 173.3451355439752\n",
      "Episode * 89/46 * Avg Reward is ==> 172.76167687368235\n",
      "Episode * 90/46 * Avg Reward is ==> 176.05786963042365\n",
      "Episode * 91/46 * Avg Reward is ==> 176.93665909948305\n",
      "Episode * 92/46 * Avg Reward is ==> 177.81537150452618\n",
      "Episode * 93/46 * Avg Reward is ==> 177.66680712923193\n",
      "Episode * 94/46 * Avg Reward is ==> 178.57643307498873\n",
      "Episode * 95/46 * Avg Reward is ==> 181.3469270255056\n",
      "Episode * 96/46 * Avg Reward is ==> 180.97371026201625\n",
      "Episode * 97/46 * Avg Reward is ==> 182.8195109086249\n",
      "Episode * 98/46 * Avg Reward is ==> 181.11462035346148\n",
      "Episode * 99/46 * Avg Reward is ==> 181.26551033395407\n",
      "Episode * 100/46 * Avg Reward is ==> 178.03395197079732\n",
      "Episode * 101/46 * Avg Reward is ==> 181.23024310708905\n",
      "Episode * 102/46 * Avg Reward is ==> 183.59995682758864\n",
      "Episode * 103/46 * Avg Reward is ==> 185.8305902189504\n",
      "Episode * 104/46 * Avg Reward is ==> 186.4534137578101\n",
      "Episode * 105/46 * Avg Reward is ==> 182.86012965400806\n",
      "Episode * 106/46 * Avg Reward is ==> 183.39810430000648\n",
      "Episode * 107/46 * Avg Reward is ==> 182.32423093309507\n",
      "Episode * 108/46 * Avg Reward is ==> 186.3014211885125\n",
      "Episode * 109/46 * Avg Reward is ==> 188.38360653290786\n",
      "Episode * 110/46 * Avg Reward is ==> 191.43006787126495\n",
      "Episode * 111/46 * Avg Reward is ==> 188.52442057381344\n",
      "Episode * 112/46 * Avg Reward is ==> 188.91218470826973\n",
      "Episode * 113/46 * Avg Reward is ==> 189.5947208600034\n",
      "Episode * 114/46 * Avg Reward is ==> 189.65513991690608\n",
      "Episode * 115/46 * Avg Reward is ==> 188.64967434142758\n",
      "Episode * 116/46 * Avg Reward is ==> 189.45762135836725\n",
      "Episode * 117/46 * Avg Reward is ==> 190.95389305929652\n",
      "Episode * 118/46 * Avg Reward is ==> 192.8661077067117\n",
      "Episode * 119/46 * Avg Reward is ==> 193.7321796047442\n",
      "Episode * 120/46 * Avg Reward is ==> 188.9712324356546\n",
      "Episode * 121/46 * Avg Reward is ==> 188.6654937858957\n",
      "Episode * 122/46 * Avg Reward is ==> 192.34762641021763\n",
      "Episode * 123/46 * Avg Reward is ==> 194.6368127772834\n",
      "Episode * 124/46 * Avg Reward is ==> 194.35809383573024\n",
      "Episode * 125/46 * Avg Reward is ==> 195.09580923653738\n",
      "Episode * 126/46 * Avg Reward is ==> 193.28453608468106\n",
      "Episode * 127/46 * Avg Reward is ==> 196.14001210093582\n",
      "Episode * 128/46 * Avg Reward is ==> 197.27159099696547\n",
      "Episode * 129/46 * Avg Reward is ==> 198.4027373917957\n",
      "Episode * 130/46 * Avg Reward is ==> 198.38108253952876\n",
      "Episode * 131/46 * Avg Reward is ==> 199.27017846202116\n",
      "Episode * 132/46 * Avg Reward is ==> 199.36990817382312\n",
      "Episode * 133/46 * Avg Reward is ==> 201.7855160416716\n",
      "Episode * 134/46 * Avg Reward is ==> 201.2151844175199\n",
      "Episode * 135/46 * Avg Reward is ==> 201.61824814603239\n",
      "Episode * 136/46 * Avg Reward is ==> 203.82260361244442\n",
      "Episode * 137/46 * Avg Reward is ==> 204.43890166202544\n",
      "Episode * 138/46 * Avg Reward is ==> 203.82459705498619\n",
      "Episode * 139/46 * Avg Reward is ==> 205.7115631551566\n",
      "Episode * 140/46 * Avg Reward is ==> 204.11915294176399\n",
      "Episode * 141/46 * Avg Reward is ==> 202.9054855288983\n",
      "Episode * 142/46 * Avg Reward is ==> 202.3069002170589\n",
      "Episode * 143/46 * Avg Reward is ==> 201.0441249547373\n",
      "Episode * 144/46 * Avg Reward is ==> 201.90364290253106\n",
      "Episode * 145/46 * Avg Reward is ==> 202.65227600231563\n",
      "Episode * 146/46 * Avg Reward is ==> 203.44425445772544\n",
      "Episode * 147/46 * Avg Reward is ==> 205.69911021803335\n",
      "Episode * 148/46 * Avg Reward is ==> 203.90370251324663\n",
      "Episode * 149/46 * Avg Reward is ==> 205.0557168451407\n",
      "Episode * 150/46 * Avg Reward is ==> 202.59583915831243\n",
      "Episode * 151/46 * Avg Reward is ==> 202.19502977734845\n",
      "Episode * 152/46 * Avg Reward is ==> 203.01748429172895\n",
      "Episode * 153/46 * Avg Reward is ==> 201.3408957364128\n",
      "Episode * 154/46 * Avg Reward is ==> 199.58585725964934\n",
      "Episode * 155/46 * Avg Reward is ==> 200.91806097837238\n",
      "Episode * 156/46 * Avg Reward is ==> 199.31652887934587\n",
      "Episode * 157/46 * Avg Reward is ==> 195.997558030564\n",
      "Episode * 158/46 * Avg Reward is ==> 193.97124559131566\n",
      "Episode * 159/46 * Avg Reward is ==> 192.7959665606249\n",
      "Episode * 160/46 * Avg Reward is ==> 195.82574605874362\n",
      "Episode * 161/46 * Avg Reward is ==> 199.82830438493687\n",
      "Episode * 162/46 * Avg Reward is ==> 195.78117285320613\n",
      "Episode * 163/46 * Avg Reward is ==> 194.1821110392611\n",
      "Episode * 164/46 * Avg Reward is ==> 193.3797220177425\n",
      "Episode * 165/46 * Avg Reward is ==> 192.8526660196215\n",
      "Episode * 166/46 * Avg Reward is ==> 194.47503683715462\n",
      "Episode * 167/46 * Avg Reward is ==> 196.61428661992034\n",
      "Episode * 168/46 * Avg Reward is ==> 195.09918007864547\n",
      "Episode * 169/46 * Avg Reward is ==> 194.83341552054813\n",
      "Episode * 170/46 * Avg Reward is ==> 193.15428946318573\n",
      "Episode * 171/46 * Avg Reward is ==> 196.70177281780096\n",
      "Episode * 172/46 * Avg Reward is ==> 198.54160590195178\n",
      "Episode * 173/46 * Avg Reward is ==> 198.13676255983472\n",
      "Episode * 174/46 * Avg Reward is ==> 196.185670843505\n",
      "Episode * 175/46 * Avg Reward is ==> 197.42411461124476\n",
      "Episode * 176/46 * Avg Reward is ==> 193.52085655427027\n",
      "Episode * 177/46 * Avg Reward is ==> 190.72163316685268\n",
      "Episode * 178/46 * Avg Reward is ==> 192.78820911619914\n",
      "Episode * 179/46 * Avg Reward is ==> 194.3138251724613\n",
      "Episode * 180/46 * Avg Reward is ==> 196.15459917635872\n",
      "Episode * 181/46 * Avg Reward is ==> 195.13458568902803\n",
      "Episode * 182/46 * Avg Reward is ==> 198.915917287378\n",
      "Episode * 183/46 * Avg Reward is ==> 198.53472683308232\n",
      "Episode * 184/46 * Avg Reward is ==> 197.44293617585691\n",
      "Episode * 185/46 * Avg Reward is ==> 202.50948393150472\n",
      "Episode * 186/46 * Avg Reward is ==> 201.34407418046501\n",
      "Episode * 187/46 * Avg Reward is ==> 198.12613946479843\n",
      "Episode * 188/46 * Avg Reward is ==> 198.2442049806566\n",
      "Episode * 189/46 * Avg Reward is ==> 197.86200213848898\n",
      "Episode * 190/46 * Avg Reward is ==> 198.06845755607273\n",
      "Episode * 191/46 * Avg Reward is ==> 201.16527467048212\n",
      "Episode * 192/46 * Avg Reward is ==> 196.77565466662452\n",
      "Episode * 193/46 * Avg Reward is ==> 195.88914255790002\n",
      "Episode * 194/46 * Avg Reward is ==> 197.27305188349368\n",
      "Episode * 195/46 * Avg Reward is ==> 200.04945766348857\n",
      "Episode * 196/46 * Avg Reward is ==> 199.45438853498237\n",
      "Episode * 197/46 * Avg Reward is ==> 200.52508574990392\n",
      "Episode * 198/46 * Avg Reward is ==> 199.39330954021983\n",
      "Episode * 199/46 * Avg Reward is ==> 201.77016304850665\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))\n",
    "\n",
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(num_actions, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() #+ noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]\n",
    "\n",
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 200\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "buffer = Buffer(50000, 64)\n",
    "\n",
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "\n",
    "# Takes about 4 min to train\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    steps = 0\n",
    "    while True:\n",
    "        # Uncomment this to see the Actor in action\n",
    "        # But not in a python notebook.\n",
    "        # env.render()\n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)[0]\n",
    "        # Recieve state and reward from environment.\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "        steps += 1\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = §np.mean(ep_reward_list[-40:])\n",
    "    print(\"Episode * {}/{} * Avg Reward is ==> {}\".format(ep, steps, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d7053eaf1f44e4ba09689f8d46ffe60bb595916505f14727b0e14a5d0bba04d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('3.9.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
