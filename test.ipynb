{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.environments.create_env import create_env\n",
    "from src.samplers.load_samplers import load_samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading funky environment (this may take a hot minute)\n",
      "Size of State Space ->  110\n",
      "Size of Action Space ->  44\n",
      "Max Value of Action ->  10.0\n",
      "Min Value of Action ->  -5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# path_to_this_notebook = os.path.abspath('.')\n",
    "# path_to_project = path_to_this_notebook[:path_to_this_notebook.find('src')]\n",
    "# sys.path.append(path_to_project)\n",
    "config = {'path_to_data':   './data/',\n",
    "          't0_hr': 6.,  # When the episode start (default value 6AM)\n",
    "          'dt_min': 30,  # Timestep size\n",
    "          'ev_dt_min': 60,  # Timestep size for EV arrivals\n",
    "          'ev_sampling_dt_min': 60,  # How EV sessions are sampled from the data\n",
    "          'apply_gaussian_noise': False,  # Make data noisy\n",
    "          'ev_utility_coef_mean': 1,  # Mean value of the utility coefficient for the EVs\n",
    "          'ev_utility_coef_scale': 0.13,  # STD of the utility coefficient for the EVs\n",
    "          'days_per_month_train': 20,  # Days per month for training\n",
    "          'ev_session_months_train': ['01', '02', '03', '04', '06', '07', '08', '09', '10', '11', ],\n",
    "          # Months to sample EV sessions for training\n",
    "          'grid_to_use': 'ieee16',  # What grid topology to use. Now supports only IEEE16.\n",
    "          'ev_session_months_test': ['05', '12'],  # Months to sample EV sessions for test\n",
    "          'n_ps_pvs': 4,  # Amount of solar panels that use PecanStreet data\n",
    "          'n_canopy_pvs': 0,  # Amount of solar panels that use canopy data\n",
    "          'canopy_pv_rated_power': 250,  # Rated power of these panels\n",
    "          'n_loads': 0,  # Amount of inflexible loads\n",
    "          'n_feeders': 1,  # Amount of feeders\n",
    "          'n_ev_chargers': 4,  # Amount of EV chargers\n",
    "\n",
    "          'ps_pvs_rated_power': 4,  # Rated power of these panels\n",
    "          'avg_evs_per_day': 3.5,  # Scaling of the EV arrival rate\n",
    "          'feeder_p_min': -5,  # Capacity of the feeders\n",
    "          'g': 4,  # Conductance of each line\n",
    "          'i_max': 25,  # Capacity of each line\n",
    "          }\n",
    "\n",
    "\n",
    "def env_creator(env_config):\n",
    "    # Preload samplers, it is necessary to avoid re-loading data each time env is created\n",
    "    (ps_samplers_dict, ps_metadata, canopy_sampler, canopy_metadata,\n",
    "     price_sampler, price_metadata, ev_sampler, elaadnl_metadata) = load_samplers(env_config)\n",
    "\n",
    "    return create_env(\n",
    "        env_config,\n",
    "        ps_samplers_dict,\n",
    "        ps_metadata,\n",
    "        canopy_sampler,\n",
    "        canopy_metadata,\n",
    "        price_sampler,\n",
    "        price_metadata,\n",
    "        ev_sampler,\n",
    "        elaadnl_metadata\n",
    "    )  # return an env instance\n",
    "\n",
    "print('Loading funky environment (this may take a hot minute)')\n",
    "env = env_creator(config)\n",
    "\n",
    "# problem = \"Pendulum-v1\"\n",
    "# env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0/46 * Avg Reward is ==> 100.27446735178185\n",
      "Episode * 1/46 * Avg Reward is ==> 164.0667934232497\n",
      "Episode * 2/46 * Avg Reward is ==> 185.0391681581144\n",
      "Episode * 3/46 * Avg Reward is ==> 189.9807860178283\n",
      "Episode * 4/46 * Avg Reward is ==> 177.72040547353316\n",
      "Episode * 5/46 * Avg Reward is ==> 163.04668037850766\n",
      "Episode * 6/46 * Avg Reward is ==> 166.01192639046243\n",
      "Episode * 7/46 * Avg Reward is ==> 171.14726104756102\n",
      "Episode * 8/46 * Avg Reward is ==> 178.90867038589323\n",
      "Episode * 9/46 * Avg Reward is ==> 175.1516616740738\n",
      "Episode * 10/46 * Avg Reward is ==> 167.22489035866536\n",
      "Episode * 11/46 * Avg Reward is ==> 166.42788478925604\n",
      "Episode * 12/46 * Avg Reward is ==> 174.25242230136655\n",
      "Episode * 13/46 * Avg Reward is ==> 166.75625671313114\n",
      "Episode * 14/46 * Avg Reward is ==> 162.11312741869224\n",
      "Episode * 15/46 * Avg Reward is ==> 168.66137365437186\n",
      "Episode * 16/46 * Avg Reward is ==> 165.6996541712562\n",
      "Episode * 17/46 * Avg Reward is ==> 165.22487206199335\n",
      "Episode * 18/46 * Avg Reward is ==> 165.72651417663883\n",
      "Episode * 19/46 * Avg Reward is ==> 176.60782826129784\n",
      "Episode * 20/46 * Avg Reward is ==> 176.01452205634888\n",
      "Episode * 21/46 * Avg Reward is ==> 177.9524916347036\n",
      "Episode * 22/46 * Avg Reward is ==> 178.35482931379164\n",
      "Episode * 23/46 * Avg Reward is ==> 177.79809205771497\n",
      "Episode * 24/46 * Avg Reward is ==> 178.05352218870902\n",
      "Episode * 25/46 * Avg Reward is ==> 175.74747027910718\n",
      "Episode * 26/46 * Avg Reward is ==> 175.8768334996087\n",
      "Episode * 27/46 * Avg Reward is ==> 172.8379742992996\n",
      "Episode * 28/46 * Avg Reward is ==> 173.68068321431178\n",
      "Episode * 29/46 * Avg Reward is ==> 172.67491905513126\n",
      "Episode * 30/46 * Avg Reward is ==> 174.96092854507367\n",
      "Episode * 31/46 * Avg Reward is ==> 178.09288743592316\n",
      "Episode * 32/46 * Avg Reward is ==> 180.2763725715425\n",
      "Episode * 33/46 * Avg Reward is ==> 178.3027370085536\n",
      "Episode * 34/46 * Avg Reward is ==> 176.0574073894374\n",
      "Episode * 35/46 * Avg Reward is ==> 175.23930025204788\n",
      "Episode * 36/46 * Avg Reward is ==> 174.71384341727384\n",
      "Episode * 37/46 * Avg Reward is ==> 174.23869961594926\n",
      "Episode * 38/46 * Avg Reward is ==> 175.18495520125467\n",
      "Episode * 39/46 * Avg Reward is ==> 174.74823594375667\n",
      "Episode * 40/46 * Avg Reward is ==> 179.92815790494072\n",
      "Episode * 41/46 * Avg Reward is ==> 179.1543086039083\n",
      "Episode * 42/46 * Avg Reward is ==> 178.41345567501585\n",
      "Episode * 43/46 * Avg Reward is ==> 176.25914438025322\n",
      "Episode * 44/46 * Avg Reward is ==> 179.55260785051476\n",
      "Episode * 45/46 * Avg Reward is ==> 182.71659583499473\n",
      "Episode * 46/46 * Avg Reward is ==> 182.4075207738612\n",
      "Episode * 47/46 * Avg Reward is ==> 181.1349786281774\n",
      "Episode * 48/46 * Avg Reward is ==> 178.88213134834314\n",
      "Episode * 49/46 * Avg Reward is ==> 181.38749200424692\n",
      "Episode * 50/46 * Avg Reward is ==> 184.44552648005592\n",
      "Episode * 51/46 * Avg Reward is ==> 184.52504905685373\n",
      "Episode * 52/46 * Avg Reward is ==> 183.45810535666823\n",
      "Episode * 53/46 * Avg Reward is ==> 188.41419137498542\n",
      "Episode * 54/46 * Avg Reward is ==> 189.04179890362374\n",
      "Episode * 55/46 * Avg Reward is ==> 186.57671713185306\n",
      "Episode * 56/46 * Avg Reward is ==> 188.23815762149553\n",
      "Episode * 57/46 * Avg Reward is ==> 187.29058261892519\n",
      "Episode * 58/46 * Avg Reward is ==> 185.638759292255\n",
      "Episode * 59/46 * Avg Reward is ==> 182.45086040099847\n",
      "Episode * 60/46 * Avg Reward is ==> 180.40227007004577\n",
      "Episode * 61/46 * Avg Reward is ==> 179.06074534025615\n",
      "Episode * 62/46 * Avg Reward is ==> 179.6345764871631\n",
      "Episode * 63/46 * Avg Reward is ==> 179.92810331613563\n",
      "Episode * 64/46 * Avg Reward is ==> 181.29882798942305\n",
      "Episode * 65/46 * Avg Reward is ==> 183.6615315192318\n",
      "Episode * 66/46 * Avg Reward is ==> 183.8559022949911\n",
      "Episode * 67/46 * Avg Reward is ==> 189.10542644563424\n",
      "Episode * 68/46 * Avg Reward is ==> 187.35302023500952\n",
      "Episode * 69/46 * Avg Reward is ==> 188.6576746304168\n",
      "Episode * 70/46 * Avg Reward is ==> 186.380891671487\n",
      "Episode * 71/46 * Avg Reward is ==> 184.4036743692265\n",
      "Episode * 72/46 * Avg Reward is ==> 181.7937196544748\n",
      "Episode * 73/46 * Avg Reward is ==> 183.8077331500681\n",
      "Episode * 74/46 * Avg Reward is ==> 185.83383658462904\n",
      "Episode * 75/46 * Avg Reward is ==> 187.4604522593257\n",
      "Episode * 76/46 * Avg Reward is ==> 187.36465044197956\n",
      "Episode * 77/46 * Avg Reward is ==> 189.19946493161504\n",
      "Episode * 78/46 * Avg Reward is ==> 188.89997867556818\n",
      "Episode * 79/46 * Avg Reward is ==> 191.66842126271814\n",
      "Episode * 80/46 * Avg Reward is ==> 188.77158181468113\n",
      "Episode * 81/46 * Avg Reward is ==> 192.45279900348328\n",
      "Episode * 82/46 * Avg Reward is ==> 192.65804543757338\n",
      "Episode * 83/46 * Avg Reward is ==> 195.17531129179142\n",
      "Episode * 84/46 * Avg Reward is ==> 194.34484240364148\n",
      "Episode * 85/46 * Avg Reward is ==> 196.71410336996883\n",
      "Episode * 86/46 * Avg Reward is ==> 195.59566390840195\n",
      "Episode * 87/46 * Avg Reward is ==> 196.5217285890695\n",
      "Episode * 88/46 * Avg Reward is ==> 194.5768404180746\n",
      "Episode * 89/46 * Avg Reward is ==> 193.26317863524025\n",
      "Episode * 90/46 * Avg Reward is ==> 192.15733370109555\n",
      "Episode * 91/46 * Avg Reward is ==> 192.66741841934694\n",
      "Episode * 92/46 * Avg Reward is ==> 189.77802737014218\n",
      "Episode * 93/46 * Avg Reward is ==> 186.23878972378347\n",
      "Episode * 94/46 * Avg Reward is ==> 189.59727515615057\n",
      "Episode * 95/46 * Avg Reward is ==> 191.53313813259595\n",
      "Episode * 96/46 * Avg Reward is ==> 192.64981330052603\n",
      "Episode * 97/46 * Avg Reward is ==> 196.79514387544765\n",
      "Episode * 98/46 * Avg Reward is ==> 199.21062427636622\n",
      "Episode * 99/46 * Avg Reward is ==> 195.82494866059318\n",
      "Episode * 100/46 * Avg Reward is ==> 199.57833309090245\n",
      "Episode * 101/46 * Avg Reward is ==> 198.83512913882956\n",
      "Episode * 102/46 * Avg Reward is ==> 199.98124230173335\n",
      "Episode * 103/46 * Avg Reward is ==> 200.1078314092419\n",
      "Episode * 104/46 * Avg Reward is ==> 201.31057115164265\n",
      "Episode * 105/46 * Avg Reward is ==> 202.79290782260622\n",
      "Episode * 106/46 * Avg Reward is ==> 206.5975203605632\n",
      "Episode * 107/46 * Avg Reward is ==> 203.60321075604634\n",
      "Episode * 108/46 * Avg Reward is ==> 205.67711016598588\n",
      "Episode * 109/46 * Avg Reward is ==> 204.96121399335146\n",
      "Episode * 110/46 * Avg Reward is ==> 206.77297328731933\n",
      "Episode * 111/46 * Avg Reward is ==> 207.8043488547724\n",
      "Episode * 112/46 * Avg Reward is ==> 207.8010536810356\n",
      "Episode * 113/46 * Avg Reward is ==> 208.7352354238294\n",
      "Episode * 114/46 * Avg Reward is ==> 207.17503322566455\n",
      "Episode * 115/46 * Avg Reward is ==> 204.83766320666578\n",
      "Episode * 116/46 * Avg Reward is ==> 205.98209137207218\n",
      "Episode * 117/46 * Avg Reward is ==> 204.10426841810573\n",
      "Episode * 118/46 * Avg Reward is ==> 202.331218404836\n",
      "Episode * 119/46 * Avg Reward is ==> 200.787313525157\n",
      "Episode * 120/46 * Avg Reward is ==> 199.60084347840709\n",
      "Episode * 121/46 * Avg Reward is ==> 196.45288542992654\n",
      "Episode * 122/46 * Avg Reward is ==> 197.0052772929559\n",
      "Episode * 123/46 * Avg Reward is ==> 194.20817005674752\n",
      "Episode * 124/46 * Avg Reward is ==> 193.01476412899427\n",
      "Episode * 125/46 * Avg Reward is ==> 188.62767422835896\n",
      "Episode * 126/46 * Avg Reward is ==> 189.9849643397602\n",
      "Episode * 127/46 * Avg Reward is ==> 189.85687740807026\n",
      "Episode * 128/46 * Avg Reward is ==> 193.05366814901404\n",
      "Episode * 129/46 * Avg Reward is ==> 191.74836265286618\n",
      "Episode * 130/46 * Avg Reward is ==> 193.22841347066415\n",
      "Episode * 131/46 * Avg Reward is ==> 195.92784868548196\n",
      "Episode * 132/46 * Avg Reward is ==> 195.99948323594032\n",
      "Episode * 133/46 * Avg Reward is ==> 196.95549520125172\n",
      "Episode * 134/46 * Avg Reward is ==> 194.90190733581568\n",
      "Episode * 135/46 * Avg Reward is ==> 193.97021497606926\n",
      "Episode * 136/46 * Avg Reward is ==> 192.90385077841492\n",
      "Episode * 137/46 * Avg Reward is ==> 188.83208472904204\n",
      "Episode * 138/46 * Avg Reward is ==> 189.00339480626022\n",
      "Episode * 139/46 * Avg Reward is ==> 189.34391727296605\n",
      "Episode * 140/46 * Avg Reward is ==> 188.4850241015474\n",
      "Episode * 141/46 * Avg Reward is ==> 189.4257882360886\n",
      "Episode * 142/46 * Avg Reward is ==> 187.87347839968348\n",
      "Episode * 143/46 * Avg Reward is ==> 187.91281001782673\n",
      "Episode * 144/46 * Avg Reward is ==> 184.49574544769587\n",
      "Episode * 145/46 * Avg Reward is ==> 181.6188679938329\n",
      "Episode * 146/46 * Avg Reward is ==> 179.16217483122955\n",
      "Episode * 147/46 * Avg Reward is ==> 178.17222323445537\n",
      "Episode * 148/46 * Avg Reward is ==> 178.87427699296816\n",
      "Episode * 149/46 * Avg Reward is ==> 177.09461145439042\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))\n",
    "\n",
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(num_actions, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() #+ noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]\n",
    "\n",
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 150\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "buffer = Buffer(50000, 64)\n",
    "\n",
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "\n",
    "# Takes about 4 min to train\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    steps = 0\n",
    "    while True:\n",
    "        # Uncomment this to see the Actor in action\n",
    "        # But not in a python notebook.\n",
    "        # env.render()\n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)[0]\n",
    "        # Recieve state and reward from environment.\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "        steps += 1\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    print(\"Episode * {}/{} * Avg Reward is ==> {}\".format(ep, steps, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d7053eaf1f44e4ba09689f8d46ffe60bb595916505f14727b0e14a5d0bba04d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('3.9.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
