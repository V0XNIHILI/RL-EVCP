{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.environments.create_env import create_env\n",
    "from src.samplers.load_samplers import load_samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading funky environment (this may take a hot minute)\n",
      "Size of State Space ->  110\n",
      "Size of Action Space ->  44\n",
      "Max Value of Action ->  10.0\n",
      "Min Value of Action ->  -5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# path_to_this_notebook = os.path.abspath('.')\n",
    "# path_to_project = path_to_this_notebook[:path_to_this_notebook.find('src')]\n",
    "# sys.path.append(path_to_project)\n",
    "config = {'path_to_data':   './data/',\n",
    "          't0_hr': 6.,  # When the episode start (default value 6AM)\n",
    "          'dt_min': 30,  # Timestep size\n",
    "          'ev_dt_min': 60,  # Timestep size for EV arrivals\n",
    "          'ev_sampling_dt_min': 60,  # How EV sessions are sampled from the data\n",
    "          'apply_gaussian_noise': False,  # Make data noisy\n",
    "          'ev_utility_coef_mean': 1,  # Mean value of the utility coefficient for the EVs\n",
    "          'ev_utility_coef_scale': 0.13,  # STD of the utility coefficient for the EVs\n",
    "          'days_per_month_train': 20,  # Days per month for training\n",
    "          'ev_session_months_train': ['01', '02', '03', '04', '06', '07', '08', '09', '10', '11', ],\n",
    "          # Months to sample EV sessions for training\n",
    "          'grid_to_use': 'ieee16',  # What grid topology to use. Now supports only IEEE16.\n",
    "          'ev_session_months_test': ['05', '12'],  # Months to sample EV sessions for test\n",
    "          'n_ps_pvs': 4,  # Amount of solar panels that use PecanStreet data\n",
    "          'n_canopy_pvs': 0,  # Amount of solar panels that use canopy data\n",
    "          'canopy_pv_rated_power': 250,  # Rated power of these panels\n",
    "          'n_loads': 0,  # Amount of inflexible loads\n",
    "          'n_feeders': 1,  # Amount of feeders\n",
    "          'n_ev_chargers': 4,  # Amount of EV chargers\n",
    "\n",
    "          'ps_pvs_rated_power': 4,  # Rated power of these panels\n",
    "          'avg_evs_per_day': 3.5,  # Scaling of the EV arrival rate\n",
    "          'feeder_p_min': -5,  # Capacity of the feeders\n",
    "          'g': 4,  # Conductance of each line\n",
    "          'i_max': 25,  # Capacity of each line\n",
    "          }\n",
    "\n",
    "\n",
    "def env_creator(env_config):\n",
    "    # Preload samplers, it is necessary to avoid re-loading data each time env is created\n",
    "    (ps_samplers_dict, ps_metadata, canopy_sampler, canopy_metadata,\n",
    "     price_sampler, price_metadata, ev_sampler, elaadnl_metadata) = load_samplers(env_config)\n",
    "\n",
    "    return create_env(\n",
    "        env_config,\n",
    "        ps_samplers_dict,\n",
    "        ps_metadata,\n",
    "        canopy_sampler,\n",
    "        canopy_metadata,\n",
    "        price_sampler,\n",
    "        price_metadata,\n",
    "        ev_sampler,\n",
    "        elaadnl_metadata\n",
    "    )  # return an env instance\n",
    "\n",
    "print('Loading funky environment (this may take a hot minute)')\n",
    "env = env_creator(config)\n",
    "\n",
    "# problem = \"Pendulum-v1\"\n",
    "# env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> 121.45683250385272\n",
      "Episode * 1 * Avg Reward is ==> 150.35123205012917\n",
      "Episode * 2 * Avg Reward is ==> 185.3457014688272\n",
      "Episode * 3 * Avg Reward is ==> 221.3520662955343\n",
      "Episode * 4 * Avg Reward is ==> 222.75030149953378\n",
      "Episode * 5 * Avg Reward is ==> 206.08470452080607\n",
      "Episode * 6 * Avg Reward is ==> 218.96626529063087\n",
      "Episode * 7 * Avg Reward is ==> 226.39734632786616\n",
      "Episode * 8 * Avg Reward is ==> 218.93623600274907\n",
      "Episode * 9 * Avg Reward is ==> 223.83722294738791\n",
      "Episode * 10 * Avg Reward is ==> 224.04757246979372\n",
      "Episode * 11 * Avg Reward is ==> 228.8405957698288\n",
      "Episode * 12 * Avg Reward is ==> 232.26833605170742\n",
      "Episode * 13 * Avg Reward is ==> 237.14934090731114\n",
      "Episode * 14 * Avg Reward is ==> 242.2376153592012\n",
      "Episode * 15 * Avg Reward is ==> 238.64848757173428\n",
      "Episode * 16 * Avg Reward is ==> 243.37631074362798\n",
      "Episode * 17 * Avg Reward is ==> 241.8371421582442\n",
      "Episode * 18 * Avg Reward is ==> 242.84896681312566\n",
      "Episode * 19 * Avg Reward is ==> 239.56227314594267\n",
      "Episode * 20 * Avg Reward is ==> 242.05072940492852\n",
      "Episode * 21 * Avg Reward is ==> 241.45648249046914\n",
      "Episode * 22 * Avg Reward is ==> 240.70364594977585\n",
      "Episode * 23 * Avg Reward is ==> 238.62969474939078\n",
      "Episode * 24 * Avg Reward is ==> 235.89464287641934\n",
      "Episode * 25 * Avg Reward is ==> 237.63161677470666\n",
      "Episode * 26 * Avg Reward is ==> 236.1278382020025\n",
      "Episode * 27 * Avg Reward is ==> 234.49840268067874\n",
      "Episode * 28 * Avg Reward is ==> 232.9093644026379\n",
      "Episode * 29 * Avg Reward is ==> 233.35692359008513\n",
      "Episode * 30 * Avg Reward is ==> 235.0294269677787\n",
      "Episode * 31 * Avg Reward is ==> 233.18968356017132\n",
      "Episode * 32 * Avg Reward is ==> 230.98587178895215\n",
      "Episode * 33 * Avg Reward is ==> 231.07343616383477\n",
      "Episode * 34 * Avg Reward is ==> 231.29218061443575\n",
      "Episode * 35 * Avg Reward is ==> 230.207883861281\n",
      "Episode * 36 * Avg Reward is ==> 229.49251163330302\n",
      "Episode * 37 * Avg Reward is ==> 228.9944969735071\n",
      "Episode * 38 * Avg Reward is ==> 228.06592406585338\n",
      "Episode * 39 * Avg Reward is ==> 228.27809034375386\n",
      "Episode * 40 * Avg Reward is ==> 233.12850674667908\n",
      "Episode * 41 * Avg Reward is ==> 233.6750059126197\n",
      "Episode * 42 * Avg Reward is ==> 232.49961641773166\n",
      "Episode * 43 * Avg Reward is ==> 230.90005123439605\n",
      "Episode * 44 * Avg Reward is ==> 230.47940365412597\n",
      "Episode * 45 * Avg Reward is ==> 229.9680732547285\n",
      "Episode * 46 * Avg Reward is ==> 232.7145660824339\n",
      "Episode * 47 * Avg Reward is ==> 233.99180086361474\n",
      "Episode * 48 * Avg Reward is ==> 235.63256238730983\n",
      "Episode * 49 * Avg Reward is ==> 236.00251235465302\n",
      "Episode * 50 * Avg Reward is ==> 236.73848215285133\n",
      "Episode * 51 * Avg Reward is ==> 236.26437061942192\n",
      "Episode * 52 * Avg Reward is ==> 237.97697104760408\n",
      "Episode * 53 * Avg Reward is ==> 233.87243396002182\n",
      "Episode * 54 * Avg Reward is ==> 233.71845875623004\n",
      "Episode * 55 * Avg Reward is ==> 234.99526145973581\n",
      "Episode * 56 * Avg Reward is ==> 233.03275310620455\n",
      "Episode * 57 * Avg Reward is ==> 233.13134000630217\n",
      "Episode * 58 * Avg Reward is ==> 232.36437172672208\n",
      "Episode * 59 * Avg Reward is ==> 233.0851841432233\n",
      "Episode * 60 * Avg Reward is ==> 235.4878356223344\n",
      "Episode * 61 * Avg Reward is ==> 235.8109644659312\n",
      "Episode * 62 * Avg Reward is ==> 235.88319022883258\n",
      "Episode * 63 * Avg Reward is ==> 234.5569538004921\n",
      "Episode * 64 * Avg Reward is ==> 235.9954237459794\n",
      "Episode * 65 * Avg Reward is ==> 233.24470123933588\n",
      "Episode * 66 * Avg Reward is ==> 234.77388402177976\n",
      "Episode * 67 * Avg Reward is ==> 233.6224704548875\n",
      "Episode * 68 * Avg Reward is ==> 232.58411276016142\n",
      "Episode * 69 * Avg Reward is ==> 233.80022183854197\n",
      "Episode * 70 * Avg Reward is ==> 231.87912124031055\n",
      "Episode * 71 * Avg Reward is ==> 233.70332320918473\n",
      "Episode * 72 * Avg Reward is ==> 235.46013904672935\n",
      "Episode * 73 * Avg Reward is ==> 236.24483945631818\n",
      "Episode * 74 * Avg Reward is ==> 234.03266050713015\n",
      "Episode * 75 * Avg Reward is ==> 235.02097216600941\n",
      "Episode * 76 * Avg Reward is ==> 232.12747468860317\n",
      "Episode * 77 * Avg Reward is ==> 231.237892127753\n",
      "Episode * 78 * Avg Reward is ==> 234.59927378123794\n",
      "Episode * 79 * Avg Reward is ==> 233.2119495709284\n",
      "Episode * 80 * Avg Reward is ==> 230.02579505500267\n",
      "Episode * 81 * Avg Reward is ==> 230.11216576763886\n",
      "Episode * 82 * Avg Reward is ==> 230.3525034459846\n",
      "Episode * 83 * Avg Reward is ==> 230.9788851027892\n",
      "Episode * 84 * Avg Reward is ==> 232.52130256293103\n",
      "Episode * 85 * Avg Reward is ==> 236.6313976182545\n",
      "Episode * 86 * Avg Reward is ==> 230.21331074311493\n",
      "Episode * 87 * Avg Reward is ==> 228.92093441484718\n",
      "Episode * 88 * Avg Reward is ==> 228.40718339336928\n",
      "Episode * 89 * Avg Reward is ==> 224.60394064194216\n",
      "Episode * 90 * Avg Reward is ==> 223.5940758348698\n",
      "Episode * 91 * Avg Reward is ==> 222.30336948176054\n",
      "Episode * 92 * Avg Reward is ==> 220.44510634526924\n",
      "Episode * 93 * Avg Reward is ==> 223.73546202590256\n",
      "Episode * 94 * Avg Reward is ==> 220.77527643964996\n",
      "Episode * 95 * Avg Reward is ==> 220.7937578697679\n",
      "Episode * 96 * Avg Reward is ==> 223.9966902287287\n",
      "Episode * 97 * Avg Reward is ==> 224.61752373675108\n",
      "Episode * 98 * Avg Reward is ==> 221.94592756270958\n",
      "Episode * 99 * Avg Reward is ==> 221.50595271650144\n",
      "Episode * 100 * Avg Reward is ==> 217.49092646150976\n",
      "Episode * 101 * Avg Reward is ==> 214.9047154654433\n",
      "Episode * 102 * Avg Reward is ==> 213.15361843885665\n",
      "Episode * 103 * Avg Reward is ==> 215.12079435102706\n",
      "Episode * 104 * Avg Reward is ==> 214.56760467661834\n",
      "Episode * 105 * Avg Reward is ==> 217.29205673880634\n",
      "Episode * 106 * Avg Reward is ==> 217.1131497108573\n",
      "Episode * 107 * Avg Reward is ==> 220.09570722946304\n",
      "Episode * 108 * Avg Reward is ==> 220.70853150262172\n",
      "Episode * 109 * Avg Reward is ==> 218.30134238030487\n",
      "Episode * 110 * Avg Reward is ==> 216.44953563553332\n",
      "Episode * 111 * Avg Reward is ==> 215.1732039137343\n",
      "Episode * 112 * Avg Reward is ==> 216.27177552368312\n",
      "Episode * 113 * Avg Reward is ==> 217.55885790301758\n",
      "Episode * 114 * Avg Reward is ==> 219.6412231864163\n",
      "Episode * 115 * Avg Reward is ==> 218.87810657810078\n",
      "Episode * 116 * Avg Reward is ==> 222.2490244746876\n",
      "Episode * 117 * Avg Reward is ==> 224.2785441026684\n",
      "Episode * 118 * Avg Reward is ==> 219.71557270964368\n",
      "Episode * 119 * Avg Reward is ==> 221.15198307942282\n",
      "Episode * 120 * Avg Reward is ==> 222.46710442126124\n",
      "Episode * 121 * Avg Reward is ==> 221.8006405577049\n",
      "Episode * 122 * Avg Reward is ==> 220.791721339302\n",
      "Episode * 123 * Avg Reward is ==> 219.51624133615186\n",
      "Episode * 124 * Avg Reward is ==> 218.23074806306892\n",
      "Episode * 125 * Avg Reward is ==> 216.26928984699958\n",
      "Episode * 126 * Avg Reward is ==> 220.89510290929502\n",
      "Episode * 127 * Avg Reward is ==> 218.35780066092093\n",
      "Episode * 128 * Avg Reward is ==> 217.69555750524492\n",
      "Episode * 129 * Avg Reward is ==> 221.24040255564154\n",
      "Episode * 130 * Avg Reward is ==> 220.35694537633844\n",
      "Episode * 131 * Avg Reward is ==> 219.07829955001415\n",
      "Episode * 132 * Avg Reward is ==> 218.7791993880287\n",
      "Episode * 133 * Avg Reward is ==> 218.5168730188271\n",
      "Episode * 134 * Avg Reward is ==> 217.78504186576592\n",
      "Episode * 135 * Avg Reward is ==> 217.4257084261592\n",
      "Episode * 136 * Avg Reward is ==> 215.985752600874\n",
      "Episode * 137 * Avg Reward is ==> 216.361512638749\n",
      "Episode * 138 * Avg Reward is ==> 218.18652273276493\n",
      "Episode * 139 * Avg Reward is ==> 218.2001010435907\n",
      "Episode * 140 * Avg Reward is ==> 216.07230502372363\n",
      "Episode * 141 * Avg Reward is ==> 216.77351804854402\n",
      "Episode * 142 * Avg Reward is ==> 218.3865904513238\n",
      "Episode * 143 * Avg Reward is ==> 219.49870301658638\n",
      "Episode * 144 * Avg Reward is ==> 218.60027811788186\n",
      "Episode * 145 * Avg Reward is ==> 219.07295435568057\n",
      "Episode * 146 * Avg Reward is ==> 218.07114963676855\n",
      "Episode * 147 * Avg Reward is ==> 215.81257015709463\n",
      "Episode * 148 * Avg Reward is ==> 215.6250876529024\n",
      "Episode * 149 * Avg Reward is ==> 216.82753374324088\n",
      "Episode * 150 * Avg Reward is ==> 220.39268013984474\n",
      "Episode * 151 * Avg Reward is ==> 221.12630882549584\n",
      "Episode * 152 * Avg Reward is ==> 221.9604883812684\n",
      "Episode * 153 * Avg Reward is ==> 218.69428294135264\n",
      "Episode * 154 * Avg Reward is ==> 217.82032909060572\n",
      "Episode * 155 * Avg Reward is ==> 217.61173849351385\n",
      "Episode * 156 * Avg Reward is ==> 216.69840077224436\n",
      "Episode * 157 * Avg Reward is ==> 214.59013721377087\n",
      "Episode * 158 * Avg Reward is ==> 217.17365413679946\n",
      "Episode * 159 * Avg Reward is ==> 217.95852661509917\n",
      "Episode * 160 * Avg Reward is ==> 218.36852467326236\n",
      "Episode * 161 * Avg Reward is ==> 220.63591872369662\n",
      "Episode * 162 * Avg Reward is ==> 221.14989994159555\n",
      "Episode * 163 * Avg Reward is ==> 220.12425321944426\n",
      "Episode * 164 * Avg Reward is ==> 219.75306326943004\n",
      "Episode * 165 * Avg Reward is ==> 219.72690368463083\n",
      "Episode * 166 * Avg Reward is ==> 215.69201547879265\n",
      "Episode * 167 * Avg Reward is ==> 215.51444981131644\n",
      "Episode * 168 * Avg Reward is ==> 216.83105663179913\n",
      "Episode * 169 * Avg Reward is ==> 215.60948127948618\n",
      "Episode * 170 * Avg Reward is ==> 217.8773477132339\n",
      "Episode * 171 * Avg Reward is ==> 217.11473482521606\n",
      "Episode * 172 * Avg Reward is ==> 214.4411160614462\n",
      "Episode * 173 * Avg Reward is ==> 213.8676980637998\n",
      "Episode * 174 * Avg Reward is ==> 216.29025421377673\n",
      "Episode * 175 * Avg Reward is ==> 214.41151412024956\n",
      "Episode * 176 * Avg Reward is ==> 211.44319212233012\n",
      "Episode * 177 * Avg Reward is ==> 210.77809577764862\n",
      "Episode * 178 * Avg Reward is ==> 211.10081726467806\n",
      "Episode * 179 * Avg Reward is ==> 211.13990913016534\n",
      "Episode * 180 * Avg Reward is ==> 212.84744567077678\n",
      "Episode * 181 * Avg Reward is ==> 215.1263509537759\n",
      "Episode * 182 * Avg Reward is ==> 217.4367712074931\n",
      "Episode * 183 * Avg Reward is ==> 214.29883780234726\n",
      "Episode * 184 * Avg Reward is ==> 215.10304858236972\n",
      "Episode * 185 * Avg Reward is ==> 212.42446917820024\n",
      "Episode * 186 * Avg Reward is ==> 212.78054358232492\n",
      "Episode * 187 * Avg Reward is ==> 212.29974392609975\n",
      "Episode * 188 * Avg Reward is ==> 216.23699331903626\n",
      "Episode * 189 * Avg Reward is ==> 217.28307693228234\n",
      "Episode * 190 * Avg Reward is ==> 214.1307889501254\n",
      "Episode * 191 * Avg Reward is ==> 217.27215694258228\n",
      "Episode * 192 * Avg Reward is ==> 214.00764290620745\n",
      "Episode * 193 * Avg Reward is ==> 213.04386238001968\n",
      "Episode * 194 * Avg Reward is ==> 212.13113084852648\n",
      "Episode * 195 * Avg Reward is ==> 213.4636029296871\n",
      "Episode * 196 * Avg Reward is ==> 213.21639752876735\n",
      "Episode * 197 * Avg Reward is ==> 213.04276976398538\n",
      "Episode * 198 * Avg Reward is ==> 214.76065356142644\n",
      "Episode * 199 * Avg Reward is ==> 212.88105968277392\n",
      "Episode * 200 * Avg Reward is ==> 214.25525377037675\n",
      "Episode * 201 * Avg Reward is ==> 212.2929981299886\n",
      "Episode * 202 * Avg Reward is ==> 213.26192090928734\n",
      "Episode * 203 * Avg Reward is ==> 212.5636073696468\n",
      "Episode * 204 * Avg Reward is ==> 211.9854971697909\n",
      "Episode * 205 * Avg Reward is ==> 211.61864251827078\n",
      "Episode * 206 * Avg Reward is ==> 210.88503218204386\n",
      "Episode * 207 * Avg Reward is ==> 210.07909456439825\n",
      "Episode * 208 * Avg Reward is ==> 212.98676940244462\n",
      "Episode * 209 * Avg Reward is ==> 209.96099234704462\n",
      "Episode * 210 * Avg Reward is ==> 207.19455245072845\n",
      "Episode * 211 * Avg Reward is ==> 209.67795224387402\n",
      "Episode * 212 * Avg Reward is ==> 210.25487865741428\n",
      "Episode * 213 * Avg Reward is ==> 209.7630318276024\n",
      "Episode * 214 * Avg Reward is ==> 209.85188155573792\n",
      "Episode * 215 * Avg Reward is ==> 212.55450119667051\n",
      "Episode * 216 * Avg Reward is ==> 212.53149332223137\n",
      "Episode * 217 * Avg Reward is ==> 211.7175029881593\n",
      "Episode * 218 * Avg Reward is ==> 211.10832042604437\n",
      "Episode * 219 * Avg Reward is ==> 212.2081561560201\n",
      "Episode * 220 * Avg Reward is ==> 212.84391040652926\n",
      "Episode * 221 * Avg Reward is ==> 211.97886800282123\n",
      "Episode * 222 * Avg Reward is ==> 212.5920624562537\n",
      "Episode * 223 * Avg Reward is ==> 215.8628119580556\n",
      "Episode * 224 * Avg Reward is ==> 218.57562541913276\n",
      "Episode * 225 * Avg Reward is ==> 220.1090129746358\n",
      "Episode * 226 * Avg Reward is ==> 223.69023187241936\n",
      "Episode * 227 * Avg Reward is ==> 223.85312990865935\n",
      "Episode * 228 * Avg Reward is ==> 218.8462277792085\n",
      "Episode * 229 * Avg Reward is ==> 217.9970360419101\n",
      "Episode * 230 * Avg Reward is ==> 217.23572347698047\n",
      "Episode * 231 * Avg Reward is ==> 212.2639987336457\n",
      "Episode * 232 * Avg Reward is ==> 214.07834829079653\n",
      "Episode * 233 * Avg Reward is ==> 215.83677372456015\n",
      "Episode * 234 * Avg Reward is ==> 219.38470403466937\n",
      "Episode * 235 * Avg Reward is ==> 217.10774016905216\n",
      "Episode * 236 * Avg Reward is ==> 216.79315882163792\n",
      "Episode * 237 * Avg Reward is ==> 216.60608412120922\n",
      "Episode * 238 * Avg Reward is ==> 212.70388863965908\n",
      "Episode * 239 * Avg Reward is ==> 212.93884884556874\n",
      "Episode * 240 * Avg Reward is ==> 210.26357444138188\n",
      "Episode * 241 * Avg Reward is ==> 209.09056050987047\n",
      "Episode * 242 * Avg Reward is ==> 207.82776544487328\n",
      "Episode * 243 * Avg Reward is ==> 211.1549108145478\n",
      "Episode * 244 * Avg Reward is ==> 213.08549289959245\n",
      "Episode * 245 * Avg Reward is ==> 214.1383143709952\n",
      "Episode * 246 * Avg Reward is ==> 214.8447231971175\n",
      "Episode * 247 * Avg Reward is ==> 218.55479870237886\n",
      "Episode * 248 * Avg Reward is ==> 215.29786672597197\n",
      "Episode * 249 * Avg Reward is ==> 219.21061005618202\n",
      "Episode * 250 * Avg Reward is ==> 220.5085495011118\n",
      "Episode * 251 * Avg Reward is ==> 223.90612117273622\n",
      "Episode * 252 * Avg Reward is ==> 227.8884070293735\n",
      "Episode * 253 * Avg Reward is ==> 229.49582471030945\n",
      "Episode * 254 * Avg Reward is ==> 233.49571326678614\n",
      "Episode * 255 * Avg Reward is ==> 235.27878918668785\n",
      "Episode * 256 * Avg Reward is ==> 236.1870957196364\n",
      "Episode * 257 * Avg Reward is ==> 237.27565203887661\n",
      "Episode * 258 * Avg Reward is ==> 237.31974265356098\n",
      "Episode * 259 * Avg Reward is ==> 235.75903540854566\n",
      "Episode * 260 * Avg Reward is ==> 233.55517926782667\n",
      "Episode * 261 * Avg Reward is ==> 234.43650326215084\n",
      "Episode * 262 * Avg Reward is ==> 232.42555496357735\n",
      "Episode * 263 * Avg Reward is ==> 231.57302006270479\n",
      "Episode * 264 * Avg Reward is ==> 231.34967982511975\n",
      "Episode * 265 * Avg Reward is ==> 229.0579131995869\n",
      "Episode * 266 * Avg Reward is ==> 226.35360158748728\n",
      "Episode * 267 * Avg Reward is ==> 230.73380899990116\n",
      "Episode * 268 * Avg Reward is ==> 231.73616010764854\n",
      "Episode * 269 * Avg Reward is ==> 230.06440905919982\n",
      "Episode * 270 * Avg Reward is ==> 231.41291740151445\n",
      "Episode * 271 * Avg Reward is ==> 231.96738262913445\n",
      "Episode * 272 * Avg Reward is ==> 229.74598698633037\n",
      "Episode * 273 * Avg Reward is ==> 228.96379110008775\n",
      "Episode * 274 * Avg Reward is ==> 226.40317161670637\n",
      "Episode * 275 * Avg Reward is ==> 227.34859304270785\n",
      "Episode * 276 * Avg Reward is ==> 230.91097349989946\n",
      "Episode * 277 * Avg Reward is ==> 231.93618812615873\n",
      "Episode * 278 * Avg Reward is ==> 235.38472512239963\n",
      "Episode * 279 * Avg Reward is ==> 237.4253186597138\n",
      "Episode * 280 * Avg Reward is ==> 237.5576284675407\n",
      "Episode * 281 * Avg Reward is ==> 241.28993799266226\n",
      "Episode * 282 * Avg Reward is ==> 240.58864412740112\n",
      "Episode * 283 * Avg Reward is ==> 237.3633300298523\n",
      "Episode * 284 * Avg Reward is ==> 236.00016456730063\n",
      "Episode * 285 * Avg Reward is ==> 234.28692225752994\n",
      "Episode * 286 * Avg Reward is ==> 235.8709089486121\n",
      "Episode * 287 * Avg Reward is ==> 236.50913382268635\n",
      "Episode * 288 * Avg Reward is ==> 236.61882084456155\n",
      "Episode * 289 * Avg Reward is ==> 238.43979348953764\n",
      "Episode * 290 * Avg Reward is ==> 239.70744518881253\n",
      "Episode * 291 * Avg Reward is ==> 239.72872590074704\n",
      "Episode * 292 * Avg Reward is ==> 235.80254260935925\n",
      "Episode * 293 * Avg Reward is ==> 233.86846964988746\n",
      "Episode * 294 * Avg Reward is ==> 227.81529245350353\n",
      "Episode * 295 * Avg Reward is ==> 225.67373205459663\n",
      "Episode * 296 * Avg Reward is ==> 225.78297071438055\n",
      "Episode * 297 * Avg Reward is ==> 227.16865782875408\n",
      "Episode * 298 * Avg Reward is ==> 226.8336223766536\n",
      "Episode * 299 * Avg Reward is ==> 228.7931095772367\n",
      "Episode * 300 * Avg Reward is ==> 230.40257613651792\n",
      "Episode * 301 * Avg Reward is ==> 228.3598624973465\n",
      "Episode * 302 * Avg Reward is ==> 226.7698466200351\n",
      "Episode * 303 * Avg Reward is ==> 227.87576503975205\n",
      "Episode * 304 * Avg Reward is ==> 227.51657900346473\n",
      "Episode * 305 * Avg Reward is ==> 225.52826726325446\n",
      "Episode * 306 * Avg Reward is ==> 221.4866629653552\n",
      "Episode * 307 * Avg Reward is ==> 220.1800381482472\n",
      "Episode * 308 * Avg Reward is ==> 223.24809402788856\n",
      "Episode * 309 * Avg Reward is ==> 225.83076909707816\n",
      "Episode * 310 * Avg Reward is ==> 227.79161765020862\n",
      "Episode * 311 * Avg Reward is ==> 227.78864106418888\n",
      "Episode * 312 * Avg Reward is ==> 232.50143648642035\n",
      "Episode * 313 * Avg Reward is ==> 231.5590112763636\n",
      "Episode * 314 * Avg Reward is ==> 231.90746009889713\n",
      "Episode * 315 * Avg Reward is ==> 234.8239213612419\n",
      "Episode * 316 * Avg Reward is ==> 232.32881827187984\n",
      "Episode * 317 * Avg Reward is ==> 233.20704195823754\n",
      "Episode * 318 * Avg Reward is ==> 231.68984951998647\n",
      "Episode * 319 * Avg Reward is ==> 227.34403385321994\n",
      "Episode * 320 * Avg Reward is ==> 226.64588919718557\n",
      "Episode * 321 * Avg Reward is ==> 224.33028808523872\n",
      "Episode * 322 * Avg Reward is ==> 226.732194818658\n",
      "Episode * 323 * Avg Reward is ==> 225.97902481053043\n",
      "Episode * 324 * Avg Reward is ==> 228.36353938592487\n",
      "Episode * 325 * Avg Reward is ==> 230.3135876026116\n",
      "Episode * 326 * Avg Reward is ==> 232.31181910606188\n",
      "Episode * 327 * Avg Reward is ==> 228.42759379001217\n",
      "Episode * 328 * Avg Reward is ==> 228.97339877219673\n",
      "Episode * 329 * Avg Reward is ==> 223.63386445240067\n",
      "Episode * 330 * Avg Reward is ==> 221.03569207529046\n",
      "Episode * 331 * Avg Reward is ==> 216.82605291469977\n",
      "Episode * 332 * Avg Reward is ==> 219.87592890619385\n",
      "Episode * 333 * Avg Reward is ==> 220.77214308802354\n",
      "Episode * 334 * Avg Reward is ==> 222.0192861250278\n",
      "Episode * 335 * Avg Reward is ==> 220.50659313581133\n",
      "Episode * 336 * Avg Reward is ==> 218.32266027822197\n",
      "Episode * 337 * Avg Reward is ==> 215.39033922585176\n",
      "Episode * 338 * Avg Reward is ==> 214.95986364056702\n",
      "Episode * 339 * Avg Reward is ==> 216.77522747917163\n",
      "Episode * 340 * Avg Reward is ==> 216.34740458357527\n",
      "Episode * 341 * Avg Reward is ==> 216.72568179401907\n",
      "Episode * 342 * Avg Reward is ==> 217.16552016385108\n",
      "Episode * 343 * Avg Reward is ==> 214.06251593375328\n",
      "Episode * 344 * Avg Reward is ==> 212.35258736801288\n",
      "Episode * 345 * Avg Reward is ==> 216.91969730212713\n",
      "Episode * 346 * Avg Reward is ==> 218.34672839817213\n",
      "Episode * 347 * Avg Reward is ==> 218.27885927888792\n",
      "Episode * 348 * Avg Reward is ==> 216.39746328561372\n",
      "Episode * 349 * Avg Reward is ==> 215.5017109909624\n",
      "Episode * 350 * Avg Reward is ==> 216.77122533553143\n",
      "Episode * 351 * Avg Reward is ==> 218.54798150378096\n",
      "Episode * 352 * Avg Reward is ==> 213.8354867920234\n",
      "Episode * 353 * Avg Reward is ==> 214.34019478766737\n",
      "Episode * 354 * Avg Reward is ==> 212.65846958557003\n",
      "Episode * 355 * Avg Reward is ==> 211.6382752786626\n",
      "Episode * 356 * Avg Reward is ==> 210.81452203591488\n",
      "Episode * 357 * Avg Reward is ==> 210.21522633188897\n",
      "Episode * 358 * Avg Reward is ==> 208.31364795427734\n",
      "Episode * 359 * Avg Reward is ==> 212.46106606749555\n",
      "Episode * 360 * Avg Reward is ==> 213.86671701731507\n",
      "Episode * 361 * Avg Reward is ==> 212.96381788028611\n",
      "Episode * 362 * Avg Reward is ==> 214.21563524971734\n",
      "Episode * 363 * Avg Reward is ==> 216.65015875700587\n",
      "Episode * 364 * Avg Reward is ==> 216.2501524954893\n",
      "Episode * 365 * Avg Reward is ==> 216.98649872822884\n",
      "Episode * 366 * Avg Reward is ==> 214.03234044390132\n",
      "Episode * 367 * Avg Reward is ==> 215.86147337160554\n",
      "Episode * 368 * Avg Reward is ==> 212.73714092959545\n",
      "Episode * 369 * Avg Reward is ==> 215.25691794432777\n",
      "Episode * 370 * Avg Reward is ==> 216.01067387237717\n",
      "Episode * 371 * Avg Reward is ==> 215.0763100082866\n",
      "Episode * 372 * Avg Reward is ==> 215.99265403305918\n",
      "Episode * 373 * Avg Reward is ==> 214.11292808760464\n",
      "Episode * 374 * Avg Reward is ==> 216.2323927348421\n",
      "Episode * 375 * Avg Reward is ==> 216.49854711239232\n",
      "Episode * 376 * Avg Reward is ==> 218.8975882147506\n",
      "Episode * 377 * Avg Reward is ==> 219.321397338474\n",
      "Episode * 378 * Avg Reward is ==> 219.3409294135451\n",
      "Episode * 379 * Avg Reward is ==> 216.46561714305352\n",
      "Episode * 380 * Avg Reward is ==> 217.46788765330058\n",
      "Episode * 381 * Avg Reward is ==> 219.7236593645651\n",
      "Episode * 382 * Avg Reward is ==> 220.94032634209265\n",
      "Episode * 383 * Avg Reward is ==> 221.57597532763003\n",
      "Episode * 384 * Avg Reward is ==> 221.88010773309642\n",
      "Episode * 385 * Avg Reward is ==> 219.86734492230215\n",
      "Episode * 386 * Avg Reward is ==> 222.0827383695543\n",
      "Episode * 387 * Avg Reward is ==> 220.71258770964613\n",
      "Episode * 388 * Avg Reward is ==> 220.29705399319715\n",
      "Episode * 389 * Avg Reward is ==> 220.71676563540012\n",
      "Episode * 390 * Avg Reward is ==> 218.28361544087457\n",
      "Episode * 391 * Avg Reward is ==> 218.2346089819651\n",
      "Episode * 392 * Avg Reward is ==> 219.050134123194\n",
      "Episode * 393 * Avg Reward is ==> 217.91219479973947\n",
      "Episode * 394 * Avg Reward is ==> 218.44310883887792\n",
      "Episode * 395 * Avg Reward is ==> 216.7252723352045\n",
      "Episode * 396 * Avg Reward is ==> 220.96059750331614\n",
      "Episode * 397 * Avg Reward is ==> 221.24739763849226\n",
      "Episode * 398 * Avg Reward is ==> 222.80582399018402\n",
      "Episode * 399 * Avg Reward is ==> 219.96479231508138\n",
      "Episode * 400 * Avg Reward is ==> 219.62079569304396\n",
      "Episode * 401 * Avg Reward is ==> 220.57383943192377\n",
      "Episode * 402 * Avg Reward is ==> 218.24454463897382\n",
      "Episode * 403 * Avg Reward is ==> 217.92278531635094\n",
      "Episode * 404 * Avg Reward is ==> 218.57093778490844\n",
      "Episode * 405 * Avg Reward is ==> 215.25321439068702\n",
      "Episode * 406 * Avg Reward is ==> 217.2535023997316\n",
      "Episode * 407 * Avg Reward is ==> 217.1526421279786\n",
      "Episode * 408 * Avg Reward is ==> 220.04475108077423\n",
      "Episode * 409 * Avg Reward is ==> 219.75633177759437\n",
      "Episode * 410 * Avg Reward is ==> 219.4981427229327\n",
      "Episode * 411 * Avg Reward is ==> 223.54712415097546\n",
      "Episode * 412 * Avg Reward is ==> 221.785705520255\n",
      "Episode * 413 * Avg Reward is ==> 222.12231422191044\n",
      "Episode * 414 * Avg Reward is ==> 219.91117122105715\n",
      "Episode * 415 * Avg Reward is ==> 220.6885822229122\n",
      "Episode * 416 * Avg Reward is ==> 220.7509061782348\n",
      "Episode * 417 * Avg Reward is ==> 222.39447348957725\n",
      "Episode * 418 * Avg Reward is ==> 223.12403075378492\n",
      "Episode * 419 * Avg Reward is ==> 223.1078072725009\n",
      "Episode * 420 * Avg Reward is ==> 222.76950323794318\n",
      "Episode * 421 * Avg Reward is ==> 220.48045601661494\n",
      "Episode * 422 * Avg Reward is ==> 219.96187707061435\n",
      "Episode * 423 * Avg Reward is ==> 221.51846997884527\n",
      "Episode * 424 * Avg Reward is ==> 220.3387471570138\n",
      "Episode * 425 * Avg Reward is ==> 220.67263227389395\n",
      "Episode * 426 * Avg Reward is ==> 217.97803747236216\n",
      "Episode * 427 * Avg Reward is ==> 218.12135459358973\n",
      "Episode * 428 * Avg Reward is ==> 217.64998415872378\n",
      "Episode * 429 * Avg Reward is ==> 215.71931786328258\n",
      "Episode * 430 * Avg Reward is ==> 216.37748462986056\n",
      "Episode * 431 * Avg Reward is ==> 211.11939852347186\n",
      "Episode * 432 * Avg Reward is ==> 213.77754002940765\n",
      "Episode * 433 * Avg Reward is ==> 214.43445875622047\n",
      "Episode * 434 * Avg Reward is ==> 216.8605261603214\n",
      "Episode * 435 * Avg Reward is ==> 216.17162162667813\n",
      "Episode * 436 * Avg Reward is ==> 213.46484945590018\n",
      "Episode * 437 * Avg Reward is ==> 211.8555948853728\n",
      "Episode * 438 * Avg Reward is ==> 212.146195420972\n",
      "Episode * 439 * Avg Reward is ==> 217.14644569514684\n",
      "Episode * 440 * Avg Reward is ==> 218.70098179034193\n",
      "Episode * 441 * Avg Reward is ==> 218.5794297964798\n",
      "Episode * 442 * Avg Reward is ==> 220.8351486930074\n",
      "Episode * 443 * Avg Reward is ==> 221.8395657332124\n",
      "Episode * 444 * Avg Reward is ==> 217.62664771675972\n",
      "Episode * 445 * Avg Reward is ==> 220.3005792976265\n",
      "Episode * 446 * Avg Reward is ==> 218.57066927667347\n",
      "Episode * 447 * Avg Reward is ==> 222.9293953983482\n",
      "Episode * 448 * Avg Reward is ==> 222.33047650071677\n",
      "Episode * 449 * Avg Reward is ==> 222.58763539430097\n",
      "Episode * 450 * Avg Reward is ==> 225.06610074505852\n",
      "Episode * 451 * Avg Reward is ==> 222.0845351046393\n",
      "Episode * 452 * Avg Reward is ==> 221.43310596502542\n",
      "Episode * 453 * Avg Reward is ==> 220.61461502755566\n",
      "Episode * 454 * Avg Reward is ==> 220.61384069583534\n",
      "Episode * 455 * Avg Reward is ==> 218.23396253102283\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/fransdeboer/Projects/RL-EVCP/test.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 198>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/fransdeboer/Projects/RL-EVCP/test.ipynb#ch0000003?line=213'>214</a>\u001b[0m buffer\u001b[39m.\u001b[39mrecord((prev_state, action, reward, state))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/fransdeboer/Projects/RL-EVCP/test.ipynb#ch0000003?line=214'>215</a>\u001b[0m episodic_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/fransdeboer/Projects/RL-EVCP/test.ipynb#ch0000003?line=216'>217</a>\u001b[0m buffer\u001b[39m.\u001b[39;49mlearn()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/fransdeboer/Projects/RL-EVCP/test.ipynb#ch0000003?line=217'>218</a>\u001b[0m update_target(target_actor\u001b[39m.\u001b[39mvariables, actor_model\u001b[39m.\u001b[39mvariables, tau)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/fransdeboer/Projects/RL-EVCP/test.ipynb#ch0000003?line=218'>219</a>\u001b[0m update_target(target_critic\u001b[39m.\u001b[39mvariables, critic_model\u001b[39m.\u001b[39mvariables, tau)\n",
      "\u001b[1;32m/Users/fransdeboer/Projects/RL-EVCP/test.ipynb Cell 3'\u001b[0m in \u001b[0;36mBuffer.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/fransdeboer/Projects/RL-EVCP/test.ipynb#ch0000003?line=102'>103</a>\u001b[0m reward_batch \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(reward_batch, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/fransdeboer/Projects/RL-EVCP/test.ipynb#ch0000003?line=103'>104</a>\u001b[0m next_state_batch \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_state_buffer[batch_indices])\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/fransdeboer/Projects/RL-EVCP/test.ipynb#ch0000003?line=105'>106</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(state_batch, action_batch, reward_batch, next_state_batch)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2952'>2953</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2953'>2954</a>\u001b[0m   (graph_function,\n\u001b[1;32m   <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2954'>2955</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2955'>2956</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2956'>2957</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1848'>1849</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1849'>1850</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1850'>1851</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1851'>1852</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1852'>1853</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1853'>1854</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1854'>1855</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m     args,\n\u001b[1;32m   <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m     possible_gradient_type,\n\u001b[1;32m   <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     executing_eagerly)\n\u001b[1;32m   <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=506'>507</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=507'>508</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=510'>511</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=511'>512</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///Users/fransdeboer/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))\n",
    "\n",
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(num_actions, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() #+ noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]\n",
    "\n",
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 500\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "buffer = Buffer(50000, 64)\n",
    "\n",
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "\n",
    "# Takes about 4 min to train\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    steps = 0\n",
    "    while True:\n",
    "        # Uncomment this to see the Actor in action\n",
    "        # But not in a python notebook.\n",
    "        # env.render()\n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)[0]\n",
    "        # Recieve state and reward from environment.\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "        steps += 1\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    print(\"Episode * {}/{} * Avg Reward is ==> {}\".format(ep, steps, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d7053eaf1f44e4ba09689f8d46ffe60bb595916505f14727b0e14a5d0bba04d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('3.9.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
